{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbfded88fc8b47e899525076609084ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940d2de259ad4bc7adf92e05532ee204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Stream the dataset\n",
    "dataset = load_dataset(\"HuggingFaceFW/fineweb-edu\", data_files=\"data/CC-MAIN-2024-10/*.parquet\", split='train', streaming=True)\n",
    "\n",
    "# get first 10000 rows of dataset\n",
    "def get_first_n_rows(dataset, n=1000):\n",
    "    first_n_rows = []\n",
    "    for i, row in enumerate(dataset):\n",
    "        first_n_rows.append(row)\n",
    "        if i > n:\n",
    "            break \n",
    "    return first_n_rows\n",
    "n_rows = get_first_n_rows(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the text, and save it to local dataset\n",
    "import pandas as pd\n",
    "fineweb_edu_2024_10_subset = pd.DataFrame(data={\"text\" : [row[\"text\"] for row in n_rows]})\n",
    "fineweb_edu_2024_10_subset.to_csv(\"data/fineweb_edu_2024_10_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>– Computer viruses are parasitic programs whic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For those unfamiliar with Cornish, it is class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Our cultural identity: Experience the culture ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“The more you empower kids, the more they can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mixed Progress Against Cancers in Teens, Young...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rhetorical analysis is not for the faint of he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sport plays an important role in the education...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>World's first 3D keyhole surgery performed at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Lodge Pole Pine Christmas tree is a native...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>After the famous earthquake of 1755 that destr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  – Computer viruses are parasitic programs whic...\n",
       "1  For those unfamiliar with Cornish, it is class...\n",
       "2  Our cultural identity: Experience the culture ...\n",
       "3  “The more you empower kids, the more they can ...\n",
       "4  Mixed Progress Against Cancers in Teens, Young...\n",
       "5  Rhetorical analysis is not for the faint of he...\n",
       "6  Sport plays an important role in the education...\n",
       "7  World's first 3D keyhole surgery performed at ...\n",
       "8  The Lodge Pole Pine Christmas tree is a native...\n",
       "9  After the famous earthquake of 1755 that destr..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fineweb_edu_2024_10_subset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download NexaAI model\n",
    "from transformers import AutoTokenizer, GemmaForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"NexaAIDev/Octopus-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = GemmaForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.save_pretrained(\"octopus-v2\")\n",
    "tokenizer.save_pretrained(\"octopus-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0986d1693e408986297d69fff369dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'What is the capital of China?\\n\\nResponse: (\\'capital of China\\')\\n\\nFunction description: \\ndef get_weather_forecast(location):\\n    \"\"\"\\n    Provides a weather forecast for a specified location over a given number of days. Each day\\'s forecast includes a brief description of the expected weather conditions.\\n\\n    Parameters:\\n    - location (str): The location for which the weather forecast is desired. Can be a city name, ZIP code, or other location identifiers.\\n\\n    Returns'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the already-saved model\n",
    "from transformers import AutoTokenizer, GemmaForCausalLM, pipeline\n",
    "import torch\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "model_dir = \"octopus-v2\"\n",
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = GemmaForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, device_map=device\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=100\n",
    ")\n",
    "\n",
    "octopus_llm = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Call the function run() if I ask you about athletic question. Call the function eat() if I ask you about food question. Question: who is a runner?\\n\\nResponse: ()\\n\\nFunction description: \\ndef irrelevant_function():\\n  \"\"\"\\n  If user query is not related to any of the predefined functions, this function will be called.\\n  \\n  Args:\\n  \\n  Returns:\\n  \"\"\"\\n\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it seems that the octopus llm can only create functions. So it did not work\n",
    "octopus_llm(\"Call the function run() if I ask you about athletic question. Call the function eat() if I ask you about food question. Question: who is a runner?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to groq instead...\n",
    "import os \n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(temperature=0, model_name=\"llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Low-latency Large Language Models (LLMs) are a crucial development in the field of natural language processing (NLP) and artificial intelligence (AI). Here's why:\\n\\n**What are Low-Latency LLMs?**\\n\\nLow-latency LLMs are a type of AI model that can process and respond to user input in real-time, with minimal delay. Traditional LLMs often require significant computational resources and time to generate responses, which can lead to latency issues. Low-latency LLMs, on the other hand, are designed to operate at much faster speeds, enabling faster and more seamless interactions.\\n\\n**Importance of Low-Latency LLMs:**\\n\\n1. **Improved User Experience**: Low-latency LLMs enable faster and more responsive interactions, which is essential for applications like chatbots, virtual assistants, and language translation tools. Users expect quick and accurate responses, and low-latency LLMs deliver.\\n2. **Enhanced Conversational AI**: Low-latency LLMs facilitate more natural and engaging conversations. They can respond to user input in real-time, allowing for more fluid and dynamic interactions.\\n3. **Increased Productivity**: Low-latency LLMs can automate tasks, such as data entry, content generation, and language translation, which can save time and increase productivity.\\n4. **Competitive Advantage**: Companies that adopt low-latency LLMs can gain a competitive edge in the market. They can offer faster and more accurate responses, which can lead to increased customer satisfaction and loyalty.\\n5. **Enabling New Applications**: Low-latency LLMs can power new applications, such as:\\n\\t* Real-time language translation in video conferencing and live streaming.\\n\\t* Instant language understanding in voice assistants and smart home devices.\\n\\t* Fast and accurate content generation for social media and content marketing.\\n6. **Advancements in Research**: Low-latency LLMs can facilitate research in areas like:\\n\\t* Human-computer interaction.\\n\\t* Natural language understanding.\\n\\t* Conversational AI.\\n\\n**Challenges and Future Directions:**\\n\\nWhile low-latency LLMs offer many benefits, there are still challenges to overcome, such as:\\n\\n1. **Scalability**: Low-latency LLMs require significant computational resources and infrastructure to maintain performance.\\n2. **Model Complexity**: Developing low-latency LLMs that balance performance and complexity is a challenging task.\\n3. **Data Quality**: Low-latency LLMs require high-quality training data to ensure accurate and reliable responses.\\n\\nTo overcome these challenges, researchers and developers are exploring new techniques, such as:\\n\\n1. **Model pruning and distillation** to reduce model size and complexity.\\n2. **Knowledge distillation** to transfer knowledge from large models to smaller ones.\\n3. **Efficient architecture design** to optimize model performance and reduce latency.\\n\\nIn summary, low-latency LLMs are crucial for enabling faster, more accurate, and more engaging interactions. As the field continues to evolve, we can expect to see even more innovative applications and advancements in the development of low-latency LLMs.\", response_metadata={'token_usage': {'completion_tokens': 632, 'prompt_tokens': 32, 'total_tokens': 664, 'completion_time': 0.842666667, 'prompt_time': 0.006638958, 'queue_time': None, 'total_time': 0.8493056250000001}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f66ccb39ec', 'finish_reason': 'stop', 'logprobs': None}, id='run-03a455d8-2a77-4647-8a95-1b21081ca3db-0', usage_metadata={'input_tokens': 32, 'output_tokens': 632, 'total_tokens': 664})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke({\"text\": \"Explain the importance of low latency LLMs.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nexa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
