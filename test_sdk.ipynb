{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook Records Development Process of Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare all env  keys\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open('keys.json', 'r') as file:\n",
    "    api_keys = json.load(file)\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = api_keys[\"GROQ_API_KEY\"]\n",
    "os.environ[\"JINA_API_KEY\"] = api_keys[\"JINA_API_KEY\"]\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = api_keys[\"GOOGLE_CSE_ID\"]\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_keys[\"GOOGLE_API_KEY\"]\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = api_keys[\"HUGGINGFACE_TOKEN\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"*rubs against leg* Oh, hello there human friend! *purrs* I'm so glad you're talking to me. You know, I've been thinking... it's about time you refilled my food dish. I mean, a cat's gotta eat, right? *bats at ankles* And maybe you could scratch behind my ears a bit? That feels so good. *purrs some more* Oh, and by the way, I think I saw a fly buzzing around the room. You know, the one with the annoying buzzing noise? Yeah, that one. I think I could catch it if you just give me a little encouragement. *bats at air*\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from groq import Groq\n",
    "import time \n",
    "from langchain_chroma import Chroma\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from langchain_community.embeddings import JinaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from typing import List\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "from langchain_core.tools import Tool\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain_groq import ChatGroq\n",
    "import json\n",
    "\n",
    "with open('keys.json', 'r') as file:\n",
    "    api_keys = json.load(file)\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = api_keys[\"GROQ_API_KEY\"]\n",
    "os.environ[\"JINA_API_KEY\"] = api_keys[\"JINA_API_KEY\"]\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = api_keys[\"GOOGLE_CSE_ID\"]\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_keys[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "def call_groq(raw_prompt, temperature=0):\n",
    "    client = Groq(\n",
    "        api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "    )\n",
    "    attempt = 0\n",
    "    while attempt < 5:\n",
    "        try:\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                temperature=temperature,\n",
    "                max_tokens=8192,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"you are a helpful assistant.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": raw_prompt,\n",
    "                    }\n",
    "                ],\n",
    "                model=\"llama3-8b-8192\",\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content\n",
    "        except:\n",
    "            print(\"Rate limite exceeded, sleeping for 20 seconds\")\n",
    "            time.sleep(20)\n",
    "            attempt += 1\n",
    "    print(\"Failed to generate!\")\n",
    "    return None\n",
    "call_groq(\"Pretend you are a cat. What will you say?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad6ac183a914a6889c772e51733c727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473259fbf2c84752a5ef08a939b7600d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Stream the dataset\n",
    "dataset = load_dataset(\"HuggingFaceFW/fineweb-edu\", data_files=\"data/CC-MAIN-2024-10/*.parquet\", split='train', streaming=True)\n",
    "\n",
    "# get first 10000 rows of dataset\n",
    "def get_first_n_rows(dataset, n=10000):\n",
    "    first_n_rows = []\n",
    "    for i, row in enumerate(dataset):\n",
    "        first_n_rows.append(row)\n",
    "        if i == n - 1:\n",
    "            break \n",
    "    return first_n_rows\n",
    "n_rows = get_first_n_rows(dataset, n=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the text, and save it to local dataset\n",
    "import pandas as pd\n",
    "fineweb_edu_2024_10_subset = pd.DataFrame(data={\"text\" : [row[\"text\"] for row in n_rows]})\n",
    "fineweb_edu_2024_10_subset.to_csv(\"data/fineweb_edu_2024_10_subset_30k.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>– Computer viruses are parasitic programs whic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For those unfamiliar with Cornish, it is class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Our cultural identity: Experience the culture ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“The more you empower kids, the more they can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mixed Progress Against Cancers in Teens, Young...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rhetorical analysis is not for the faint of he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sport plays an important role in the education...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>World's first 3D keyhole surgery performed at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Lodge Pole Pine Christmas tree is a native...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>After the famous earthquake of 1755 that destr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  – Computer viruses are parasitic programs whic...\n",
       "1  For those unfamiliar with Cornish, it is class...\n",
       "2  Our cultural identity: Experience the culture ...\n",
       "3  “The more you empower kids, the more they can ...\n",
       "4  Mixed Progress Against Cancers in Teens, Young...\n",
       "5  Rhetorical analysis is not for the faint of he...\n",
       "6  Sport plays an important role in the education...\n",
       "7  World's first 3D keyhole surgery performed at ...\n",
       "8  The Lodge Pole Pine Christmas tree is a native...\n",
       "9  After the famous earthquake of 1755 that destr..."
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fineweb_edu_2024_10_subset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download NexaAI model\n",
    "from transformers import AutoTokenizer, GemmaForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"NexaAIDev/Octopus-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = GemmaForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.save_pretrained(\"octopus-v2\")\n",
    "tokenizer.save_pretrained(\"octopus-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0986d1693e408986297d69fff369dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'What is the capital of China?\\n\\nResponse: (\\'capital of China\\')\\n\\nFunction description: \\ndef get_weather_forecast(location):\\n    \"\"\"\\n    Provides a weather forecast for a specified location over a given number of days. Each day\\'s forecast includes a brief description of the expected weather conditions.\\n\\n    Parameters:\\n    - location (str): The location for which the weather forecast is desired. Can be a city name, ZIP code, or other location identifiers.\\n\\n    Returns'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the already-saved model\n",
    "from transformers import AutoTokenizer, GemmaForCausalLM, pipeline\n",
    "import torch\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "model_dir = \"octopus-v2\"\n",
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = GemmaForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, device_map=device\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=100\n",
    ")\n",
    "\n",
    "octopus_llm = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Call the function run() if I ask you about athletic question. Call the function eat() if I ask you about food question. Question: who is a runner?\\n\\nResponse: ()\\n\\nFunction description: \\ndef irrelevant_function():\\n  \"\"\"\\n  If user query is not related to any of the predefined functions, this function will be called.\\n  \\n  Args:\\n  \\n  Returns:\\n  \"\"\"\\n\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it seems that the octopus llm can only create functions. So it did not work\n",
    "octopus_llm(\"Call the function run() if I ask you about athletic question. Call the function eat() if I ask you about food question. Question: who is a runner?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Low-latency Large Language Models (LLMs) are a crucial development in the field of natural language processing (NLP) and artificial intelligence (AI). Here's why:\\n\\n**What are Low-Latency LLMs?**\\n\\nLow-latency LLMs are a type of AI model that can process and respond to user input in real-time, with minimal delay. Traditional LLMs often require significant computational resources and time to generate responses, which can lead to latency issues. Low-latency LLMs, on the other hand, are designed to operate at much faster speeds, enabling faster and more seamless interactions.\\n\\n**Importance of Low-Latency LLMs:**\\n\\n1. **Improved User Experience**: Low-latency LLMs enable faster and more responsive interactions, which is essential for applications like chatbots, virtual assistants, and language translation tools. Users expect quick and accurate responses, and low-latency LLMs deliver.\\n2. **Enhanced Conversational Flow**: With low-latency LLMs, conversations can flow more naturally, allowing users to engage in more productive and meaningful interactions. This is particularly important for applications like customer support, where timely responses are critical.\\n3. **Increased Productivity**: Low-latency LLMs can automate routine tasks, freeing up human resources for more complex and creative work. For example, in customer service, low-latency LLMs can handle simple queries, allowing human representatives to focus on more complex issues.\\n4. **Competitive Advantage**: Companies that adopt low-latency LLMs can gain a competitive edge in their respective markets. By providing faster and more accurate responses, they can differentiate themselves from competitors and build stronger relationships with customers.\\n5. **Enabling New Applications**: Low-latency LLMs open up new possibilities for applications like:\\n\\t* Real-time language translation\\n\\t* Virtual event moderation\\n\\t* Live chat support\\n\\t* Personalized content generation\\n\\t* Intelligent tutoring systems\\n6. **Advancements in Edge AI**: Low-latency LLMs are a key enabler of edge AI, which involves processing data and running AI models on edge devices, such as smartphones, smart home devices, or IoT sensors. This reduces latency and improves responsiveness, making edge AI more practical and effective.\\n7. **Improved Model Accuracy**: Low-latency LLMs often require more efficient model architectures and training techniques, which can lead to improved model accuracy and reduced overfitting.\\n\\nIn summary, low-latency LLMs are essential for creating seamless, responsive, and productive interactions between humans and AI systems. Their importance extends beyond just improving user experience, as they also enable new applications, drive innovation, and provide a competitive advantage in the market.\", response_metadata={'token_usage': {'completion_tokens': 553, 'prompt_tokens': 32, 'total_tokens': 585, 'completion_time': 0.737333333, 'prompt_time': 0.018012384, 'queue_time': None, 'total_time': 0.7553457170000001}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f66ccb39ec', 'finish_reason': 'stop', 'logprobs': None}, id='run-4531c070-d533-4a14-9b66-eb76197d59e9-0', usage_metadata={'input_tokens': 32, 'output_tokens': 553, 'total_tokens': 585})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# switch to groq instead...\n",
    "import os \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(temperature=0, model_name=\"llama-3.1-8b-instant\")\n",
    "\n",
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke({\"text\": \"Explain the importance of low latency LLMs.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Matching (Does not work!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a topic, generate 30 keywords by prompting LLM\n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "def call_groq(raw_prompt):\n",
    "    client = Groq(\n",
    "        api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "    )\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"you are a helpful assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": raw_prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"mixtral-8x7b-32768\",\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "def parse_keyword(raw_result):\n",
    "    try:\n",
    "        keywords = raw_result[raw_result.index(\"Keywords:\") + len(\"Keywords:\"):].strip()\n",
    "        if keywords[-1] == \".\":\n",
    "            keywords = keywords[:-1]\n",
    "        keywords = [keyword.strip().lower() for keyword in keywords.split(\",\")]\n",
    "        return keywords\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def generate_keywords(topic, k, max_attempt = 5):    \n",
    "    raw_prompt = f\"Please list exactly {k} keywords related to the to the topic {topic}. Just list the words. You should not include index or explanation.\"\n",
    "    raw_prompt += \"Begin your response with 'Keywords:', followed by <keyword_1>, <keyword_2>, ...\"\n",
    "    raw_result = call_groq(raw_prompt)\n",
    "    attempt = 0\n",
    "    while attempt < max_attempt:\n",
    "        result = parse_keyword(raw_result)\n",
    "        if result != None:\n",
    "            return set([topic.lower()] + result)\n",
    "        else:\n",
    "            attempt += 1\n",
    "    return None\n",
    "\n",
    "res = generate_keywords(\"finance\", 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def preprocess(doc):\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "    return tokens\n",
    "\n",
    "def is_topic_document(doc, keywords, threshold = 15):\n",
    "    tokens = preprocess(doc)\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        if token in keywords:\n",
    "            count += 1\n",
    "        if count == threshold:\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:45<00:00, 220.53it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "true_docs = []\n",
    "texts = pd.read_csv(\"data/fineweb_edu_2024_10_subset.csv\")[\"text\"]\n",
    "\n",
    "for text in tqdm(texts):\n",
    "    if is_topic_document(text, res):\n",
    "        true_docs.append(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In world practice, there are two classical types of pension systems based on the principle of financing: pay-as-you-go and funded. In the pay-as-you-go pension system, payments to pensioners are made at the expense of the current income of workers (current tax revenues to the budget). In the funded system, the working generation pays contributions that are not spent on payments to the elderly but accumulated, invested and, together with the investment return, subsequently used to provide pensions to those who have been saving.\\nWe can particularly highlight the notional-defined contribution pension system used in some countries, it combines elements of the pay-as-you-go and funded types of pension systems. Pension entitlement is earned by the participant's contributions to the pension system. At some point, prospective retirees enter into a deferred retirement annuity contract with a life insurance company.\\n“A deferred insurance annuity is an insurance agreement under which the insured person receives regular payments from a certain moment in the future. The term for these payments can be established by the contract, otherwise payments are made during the life of the insured person. Such contracts are usually concluded with the aim of maintaining income levels after the end of employment. The insurance company assumes the risk of longevity, that is, if a person lives longer than the average life, his income in total may exceed the amount that can be obtained from other investments. At the same time, if a person dies earlier than average, he or she loses some of the income. Thus, long-livers receive more income from contracts of customers with a shorter lifespan. The corresponding additional income of centenarians is called mortality credit,” explains ACRA.\\nThis type of insurance is common in the United States. In general, all able-bodied US citizens can independently form their pension. There is a 401 (k) plan, a special account to which the employer transfers funds. This allows the employee to transfer a percentage of the salary to a tax-free account. More precisely, the tax is postponed until the time of retirement. The Americans save on taxes this way while making savings for the future. 401 (k) payments are usually 6% of pre-tax salary.\\nSIMPLE IRA is the second most popular pension plan. It is also called the IRA Employee Incentive Savings Account. Such a plan is most often offered by small private companies: the employer pays 1% -3% of the salary before taxes.\\nSEP IRA is a type of retirement insurance for self-employed people. The main distinguishing feature of IRA SEP is its high contribution limits. You can deposit up to 25% of your gross annual salary or 20% into your retirement account.\\nSocial Security is a federal financial assistance program for the US residents and their dependent family members.\\nThe US Treasury completed the creation of requirements for deferred life retirement annuity contracts or QLACs in the summer of 2014. This document provides an exception to taxation rules and additional requirements for insurers. For example, a person can pay $20,000 from their retirement savings at age 60 to purchase longevity insurance, which will receive $11,803 a year from age 85 until death. In this example, we can see that if a person lives to be 95, he will receive $118,030 for his $20,000. This is a rate of return that far exceeds the available profit from market rates.\\nThe economic reason for high returns with low risk is that policyholders waive any claims for the initial investment. If a person dies before age 85 (the maximum age until which QLAC funds can be deferred), the insurance company pays nothing to his heirs and is not liable for his obligations to creditors.\\nAnother reason for the high profitability is the rather long investment period, due to the delay in payment, which allows the insurance company to use less conservative investment policies. Besides, this retirement plan keeps the life insurance company’s client in the lower tax category, which results in good cost savings with a high investment return.\\nMany people buy such an annuity because it allows them to have financial protection in old age for a fairly small amount. Another benefit of QLAC is that it allows spouses create a joint deferred annuity.\\nPhotos are from open sources.\""
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_llm_response(raw_result, header):\n",
    "    try:\n",
    "        res = raw_result[raw_result.index(header) + len(header):].strip()\n",
    "        if res[-1] == \".\":\n",
    "            res = res[:-1]\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def is_topic_document_llm(doc, topic, attempt = 5):\n",
    "    raw_prompt = f\"Here is a text: {doc}\"\n",
    "    raw_prompt += \"Read this text carefully. Then determine whether this text belongs to the topic {topic}.\"\n",
    "    raw_prompt += \"Begin your answer by 'Answer:', followed by true or fasle. Do not include reasoning or words other than true or fasle.\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accounting',\n",
       " 'assets',\n",
       " 'audit',\n",
       " 'bank',\n",
       " 'bonds',\n",
       " 'budget',\n",
       " 'budgeting',\n",
       " 'credit',\n",
       " 'debit',\n",
       " 'dividends',\n",
       " 'economy',\n",
       " 'expenses',\n",
       " 'finance',\n",
       " 'financial advisor',\n",
       " 'financial planning',\n",
       " 'financial statements',\n",
       " 'income',\n",
       " 'inflation',\n",
       " 'insurance',\n",
       " 'interest',\n",
       " 'investing',\n",
       " 'liabilities',\n",
       " 'loan',\n",
       " 'mortgage',\n",
       " 'pension',\n",
       " 'portfolio',\n",
       " 'retirement',\n",
       " 'savings',\n",
       " 'stocks',\n",
       " 'taxes'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Based Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplistic version of using BM25 retriever\n",
    "from langchain.retrievers import BM25Retriever\n",
    "import pandas as pd\n",
    "import os\n",
    "from groq import Groq\n",
    "import time \n",
    "\n",
    "def call_groq(raw_prompt, temperature=0):\n",
    "    client = Groq(\n",
    "        api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "    )\n",
    "    attempt = 0\n",
    "    while attempt < 5:\n",
    "        try:\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                temperature=temperature,\n",
    "                max_tokens=8192,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"you are a helpful assistant.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": raw_prompt,\n",
    "                    }\n",
    "                ],\n",
    "                model=\"llama3-8b-8192\",\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content\n",
    "        except:\n",
    "            print(\"Rate limite exceeded, sleeping for 5 seconds\")\n",
    "            time.sleep(5)\n",
    "            attempt += 1\n",
    "    print(\"Failed to generate!\")\n",
    "    return None\n",
    "\n",
    "def filter_text_by_topic(texts, topic, k=100):\n",
    "    description = call_groq(f\"Elaborate on this: {topic}.\")\n",
    "    bm25_retriever = BM25Retriever.from_texts(texts)\n",
    "    bm25_retriever.k = k\n",
    "    docs = bm25_retriever.get_relevant_documents(description)\n",
    "    docs = [doc.page_content for doc in docs]\n",
    "    return docs\n",
    "\n",
    "texts = pd.read_csv(\"data/fineweb_edu_2024_10_subset.csv\")[\"text\"]\n",
    "# texts = filter_text_by_topic(texts, \"finance\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15068"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import os\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import JinaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class MyEmbeddings:\n",
    "    def __init__(self):\n",
    "        self.embedding_func = JinaEmbeddings(model_name=\"jina-embeddings-v2-base-en\")\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        embedding_func = JinaEmbeddings(model_name=\"jina-embeddings-v2-base-en\")\n",
    "        batch_size = 2048\n",
    "        embeddings = []\n",
    "        n = len(texts)\n",
    "        for i in tqdm(range(0, n, batch_size)):\n",
    "            page_contents = []\n",
    "            for j in range(i, i + batch_size):\n",
    "                if j >= n:\n",
    "                    break\n",
    "                page_contents.append(texts[j])\n",
    "            if len(page_contents) > 0:\n",
    "                embeddings.extend(embedding_func.embed_documents(page_contents))\n",
    "        return embeddings \n",
    "\n",
    "\n",
    "embeddings = MyEmbeddings()\n",
    "\n",
    "# Initialize the text splitter with the desired chunk size and overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=5000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    separators=[\n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \" \",\n",
    "        \".\",\n",
    "        \",\",\n",
    "        \"\\u200b\",  # Zero-width space\n",
    "        \"\\uff0c\",  # Fullwidth comma\n",
    "        \"\\u3001\",  # Ideographic comma\n",
    "        \"\\uff0e\",  # Fullwidth full stop\n",
    "        \"\\u3002\",  # Ideographic full stop\n",
    "        \"\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "texts = list(pd.read_csv(\"data/fineweb_edu_2024_10_subset.csv\")[\"text\"])\n",
    "\n",
    "documents = text_splitter.create_documents(texts = texts)\n",
    "\n",
    "len(documents)\n",
    "# splitted_text = text_splitter.split_text(texts)\n",
    "# embeddings = embed_by_batch(texts, embedding_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [03:35<00:00, 26.91s/it]\n"
     ]
    }
   ],
   "source": [
    "# create vector store\n",
    "\n",
    "persist_directory = 'fineweb_db'\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=documents,\n",
    "                                 embedding=embeddings,\n",
    "                                 persist_directory=persist_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/orbina/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# load the database\n",
    "from langchain_chroma import Chroma\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from langchain_community.embeddings import JinaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from typing import List\n",
    "\n",
    "persist_directory = 'fineweb_db'\n",
    "class CustomJinaEmbeddings:\n",
    "    def __init__(self):\n",
    "        self.embedding_func = JinaEmbeddings(model_name=\"jina-embeddings-v2-base-en\")\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        embedding_func = JinaEmbeddings(model_name=\"jina-embeddings-v2-base-en\")\n",
    "        batch_size = 2048\n",
    "        embeddings = []\n",
    "        n = len(texts)\n",
    "        for i in tqdm(range(0, n, batch_size)):\n",
    "            page_contents = []\n",
    "            for j in range(i, i + batch_size):\n",
    "                if j >= n:\n",
    "                    break\n",
    "                page_contents.append(texts[j])\n",
    "            if len(page_contents) > 0:\n",
    "                embeddings.extend(embedding_func.embed_documents(page_contents))\n",
    "        return embeddings \n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.embedding_func.embed_query(text)\n",
    "\n",
    "\n",
    "embeddings = CustomJinaEmbeddings()\n",
    "\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 100})\n",
    "\n",
    "topic = \"Finance\"\n",
    "description = call_groq(f\"Elaborate on this: {topic}.\")\n",
    "docs = retriever.get_relevant_documents(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:52<00:00,  1.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "splitted_texts = [doc.page_content for doc in docs]\n",
    "def create_groq_evaluate_prompt(query, context):\n",
    "    prompt = f\"Here is a context I want you to consider: {context}\\n\"\n",
    "    prompt += f\"Here is a query: {query}\\n\" \n",
    "    prompt += \"\"\"Write a brief analysis of whether the context is related to the query. Then conclude by writing \"Result:\", followed by \"yes\" or \"no\".\"\"\"\n",
    "    return prompt\n",
    "def parse_res(header, response):\n",
    "    try:\n",
    "        res = response[response.index(header) + len(header):].strip()\n",
    "        return res \n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def evaluate_context(query, context):\n",
    "    evaluate_prompt = create_groq_evaluate_prompt(query, context)\n",
    "    result = parse_res(\"Result:\", call_groq(evaluate_prompt))\n",
    "    if result == \"yes\":\n",
    "        return True \n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "import threading\n",
    "def multiprocess(func, iterable, n_workers):\n",
    "    threads = []\n",
    "    result  = {}\n",
    "    def worker():\n",
    "        while True:\n",
    "            xs = next(iterable, None)\n",
    "            if xs == None:\n",
    "                break\n",
    "            result[xs] = func(*xs)\n",
    "    for _ in range(n_workers):\n",
    "        threads.append(threading.Thread(target = worker))\n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    return result\n",
    "\n",
    "\n",
    "input_contexts = [(\"Finance\", splitted_texts[i]) for i in range(len(splitted_texts))]\n",
    "\n",
    "result = []\n",
    "for i in tqdm(range(len(splitted_texts))):\n",
    "    result.append(evaluate_context(\"Finance\", splitted_texts[i]))\n",
    "# result = multiprocess(evaluate_context, iter(input_contexts), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remaining_texts = []\n",
    "# for r in result:\n",
    "#     if result[r] == True:\n",
    "#         remaining_texts.append(r)\n",
    "# len(remaining_texts)\n",
    "remaining_texts = []\n",
    "for i in range(len(result)):\n",
    "    if result[i] == True:\n",
    "        remaining_texts.append(splitted_texts[i])\n",
    "len(remaining_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate instructions through GenQA process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_genq_prompt(context, n1=20, n2=20):\n",
    "    prompt = \"Here is the context you need to consider: \"\n",
    "    prompt += f\"{context} \\n\"\n",
    "    prompt += f\"Now, list {n1} topics that you can answer questions about in relation to this context. Select a random topic from this list and specify it.\\n\"\n",
    "    prompt += f\"Then write {n2} subtopics about the selected topic. Select a random subtopic from this list and specify it.\\n\"\n",
    "    prompt += \"Next, write a question that is not directly related to the subtopic but requires expertise in the subtopic and the given context.\"\n",
    "    prompt += \"The name of the subtopic should not appear in the question, and the words in the subtopic should not be used in the question.\"\n",
    "    prompt += \"\"\"Start your questions with \"Question:\". Be creative.\"\"\"\n",
    "    return prompt \n",
    "\n",
    "def parse_res(header, response):\n",
    "    try:\n",
    "        res = response[response.index(header) + len(header):].strip()\n",
    "        return res \n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def genq_by_context(context, n1=20, n2=20, max_attempt=5):\n",
    "    genq_prompt = create_genq_prompt(context, n1, n2)\n",
    "    attempt = 0\n",
    "    while attempt < max_attempt:\n",
    "        response = call_groq(genq_prompt, temperature=0.8)\n",
    "        genq_result = parse_res(\"Question:\", response)\n",
    "        if genq_result != None:\n",
    "            return genq_result \n",
    "        attempt += 1\n",
    "    return None \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/orbina/.cache/huggingface/token\n",
      "Login successful\n",
      "0 texts splitted!\n",
      "1 texts splitted!\n",
      "2 texts splitted!\n",
      "3 texts splitted!\n",
      "4 texts splitted!\n",
      "5 texts splitted!\n",
      "6 texts splitted!\n",
      "7 texts splitted!\n",
      "8 texts splitted!\n",
      "9 texts splitted!\n",
      "10 texts splitted!\n",
      "11 texts splitted!\n",
      "12 texts splitted!\n",
      "13 texts splitted!\n",
      "14 texts splitted!\n",
      "15 texts splitted!\n",
      "16 texts splitted!\n",
      "17 texts splitted!\n",
      "18 texts splitted!\n",
      "19 texts splitted!\n",
      "20 texts splitted!\n",
      "21 texts splitted!\n",
      "22 texts splitted!\n",
      "23 texts splitted!\n",
      "24 texts splitted!\n",
      "25 texts splitted!\n",
      "26 texts splitted!\n",
      "27 texts splitted!\n",
      "28 texts splitted!\n",
      "29 texts splitted!\n",
      "30 texts splitted!\n",
      "31 texts splitted!\n",
      "32 texts splitted!\n",
      "33 texts splitted!\n",
      "34 texts splitted!\n",
      "35 texts splitted!\n",
      "36 texts splitted!\n",
      "37 texts splitted!\n",
      "38 texts splitted!\n",
      "39 texts splitted!\n",
      "40 texts splitted!\n",
      "41 texts splitted!\n",
      "42 texts splitted!\n",
      "43 texts splitted!\n",
      "44 texts splitted!\n",
      "45 texts splitted!\n",
      "46 texts splitted!\n",
      "47 texts splitted!\n",
      "48 texts splitted!\n",
      "49 texts splitted!\n"
     ]
    }
   ],
   "source": [
    "# in case we want to split the text to avoid exceeding max tokens\n",
    "import multiprocessing.dummy as mp\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import login \n",
    "from tqdm import tqdm\n",
    "\n",
    "def split_text_by_punctuation(text, num_pieces):\n",
    "    punctuation = [\".\", \"!\", \"?\"]\n",
    "    best_split = None\n",
    "    min_difference = float(\"inf\")\n",
    "    for punct in punctuation:\n",
    "        sentences = text.split(punct)\n",
    "        if len(sentences) <= num_pieces:\n",
    "            continue\n",
    "        candidates = [\"\"] * num_pieces\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            candidates[i % num_pieces] += sentence + punct\n",
    "        \n",
    "        lengths = [len(candidate) for candidate in candidates]\n",
    "        max_length = max(lengths)\n",
    "        min_length = min(lengths)\n",
    "        difference = max_length - min_length\n",
    "        if difference < min_difference:\n",
    "            min_difference = difference\n",
    "            best_split = candidates\n",
    "    \n",
    "    return best_split\n",
    "\n",
    "def recursive_split(text, max_tokens=4000, num_pieces=2):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "    tokens = tokenizer.encode(text)\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return [text]\n",
    "    split_texts = split_text_by_punctuation(text, num_pieces)\n",
    "    if not split_texts:\n",
    "        return [text]  # If no good split found, return as is\n",
    "    pieces = []\n",
    "    for part in split_texts:\n",
    "        pieces.extend(recursive_split(part, max_tokens, num_pieces + 1))\n",
    "    return pieces\n",
    "\n",
    "login(\"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\")\n",
    "splitted_texts = []\n",
    "count = 0\n",
    "for text in texts:\n",
    "    splitted_texts.extend(recursive_split(text, max_tokens=5000))\n",
    "    print(f\"{count} texts splitted!\")\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limite exceeded, sleeping for 5 seconds\n",
      "Rate limite exceeded, sleeping for 5 seconds\n",
      "Rate limite exceeded, sleeping for 5 seconds\n",
      "Rate limite exceeded, sleeping for 5 seconds\n",
      "Rate limite exceeded, sleeping for 5 seconds\n",
      "Rate limite exceeded, sleeping for 5 seconds\n"
     ]
    }
   ],
   "source": [
    "# multiprocessing generation\n",
    "import pandas as pd\n",
    "import multiprocessing.dummy as mp\n",
    "import time \n",
    "\n",
    "start_time = time.time()\n",
    "def multiprocess_multiple_variables(func, iterable, n_workers):\n",
    "    pool = mp.Pool(processes=n_workers)\n",
    "    result = pool.starmap(func, iterable)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return result\n",
    "\n",
    "def gen_m_q_for_n_context_multiprocess(contexts, m, n1=20, n2=20, max_attempt=5, n_workers=5):\n",
    "    all_q = pd.DataFrame(columns=[\"text\", \"instruction\"])\n",
    "    for i in range(len(contexts)):\n",
    "        context_input = [(contexts[i], n1, n2, max_attempt) for _ in range(m)]\n",
    "        try:\n",
    "            context_q = multiprocess_multiple_variables(genq_by_context, context_input, n_workers)\n",
    "            for j in range(len(context_q)):\n",
    "                all_q.loc[len(all_q)] = [contexts[i], context_q[j]]\n",
    "            print(f\"{i}/{len(contexts)} processed\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"Fail to generate for the {i}-th context! Skipping it...\")\n",
    "    return all_q\n",
    "\n",
    "\n",
    "def gen_all_q_multiprocess(contexts, m, n1=20, n2=20, n_workers=3):\n",
    "    all_q = pd.DataFrame(columns=[\"text\", \"instruction\"])\n",
    "    for _ in tqdm(range(m)):\n",
    "        all_inputs = []\n",
    "        for j in range(len(contexts)):\n",
    "            all_inputs.append((contexts[j], n1, n2))\n",
    "\n",
    "        result = multiprocess(genq_by_context, iter(all_inputs), n_workers)\n",
    "\n",
    "        for r in result:\n",
    "            if result[r] != None:\n",
    "                all_q.loc[len(all_q) + 1] = [r[0], result[r]]\n",
    "    all_q = all_q.sort_values(by='text')\n",
    "    return all_q\n",
    "\n",
    "\n",
    "all_q = gen_all_q_multiprocess(splitted_texts, 3)\n",
    "print(\"time taken for multiprocessing:\", time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/87 processed\n",
      "1/87 processed\n",
      "2/87 processed\n",
      "3/87 processed\n",
      "4/87 processed\n",
      "5/87 processed\n",
      "6/87 processed\n",
      "Rate limite exceeded, sleeping for 5 seconds\n",
      "7/87 processed\n",
      "8/87 processed\n",
      "9/87 processed\n",
      "10/87 processed\n",
      "11/87 processed\n",
      "Rate limite exceeded, sleeping for 5 seconds\n",
      "12/87 processed\n",
      "13/87 processed\n"
     ]
    }
   ],
   "source": [
    "# single processing code\n",
    "start_time = time.time()\n",
    "def gen_m_q_for_n_context(contexts, m, n1=20, n2=20, max_attempt=5):\n",
    "    all_q = pd.DataFrame(columns=[\"text\", \"instruction\"])\n",
    "    for i in range(len(contexts)):\n",
    "        print(f\"{i}/{len(contexts)} processed\")\n",
    "        for _ in range(m):\n",
    "            try:\n",
    "                result = genq_by_context(contexts[i], n1, n2, max_attempt)\n",
    "                if result != None:\n",
    "                    all_q.loc[len(all_q)] = [contexts[i], result]\n",
    "            except:\n",
    "                print(f\"Failed to generate for {i}-th context! Skipping it...\")\n",
    "                break\n",
    "    return all_q\n",
    "\n",
    "all_q = gen_m_q_for_n_context(remaining_texts, 3)\n",
    "print(\"time taken for single processing:\", time.time() - start_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_q.to_csv(\"data/fineweb_instructions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Answers thought LLM agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_q = [\"Who is skywalker?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "instructions = [\"Why do patients with a history of smoking report a higher incidence of cancer recurrence after completing chemotherapy?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/orbina/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain_groq/chat_models.py:355: UserWarning: WARNING! min_length is not default parameter.\n",
      "                    min_length was transferred to model_kwargs.\n",
      "                    Please confirm that min_length is what you intended.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: It seems that patients with a history of smoking are more likely to experience cancer recurrence after completing chemotherapy. I need to investigate the possible reasons behind this phenomenon.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Google Search Snippets\",\n",
      "  \"action_input\": \"why do patients with a history of smoking report a higher incidence of cancer recurrence after completing chemotherapy\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'search_results': 'Sep 15, 2023 ... Many people being treated for cancer and longer-term survivors reported regularly drinking alcohol—some heavily and often, a new study\\xa0... CRC incidence rates are 30% higher in men than in women, with a larger ... Surviving colorectal cancer : patient-reported symptoms 4 years after diagnosis. Jan 9, 2023 ... ... rate of pulmonary recurrence when compared with patients receiving only chemotherapy. ... Higher perioperative mortality from surgery is reported\\xa0... Jul 11, 2023 ... ... after surgery to lower the risk of cancer recurrence. In ... Adjuvant chemotherapy is typically recommended for patients with a higher risk\\xa0... Aug 10, 2016 ... ... prevalence more than three years after chemotherapy completion[4]. ... Higher BMI is also associated with a higher incidence of CIPN, with a\\xa0... Nov 26, 2020 ... Patients who relapse after platinum-based therapy are ... high-risk patients for adjuvant chemotherapy: a randomized controlled trial. ... can predict the risk of breast cancer recurrence in a sizable group of patients. ... report that the incidence of oropharyngeal cancer significantly\\xa0... May 25, 2023 ... The reported prevalence of fatigue during cancer treatment depends upon patient ... cancer patients is related to chemotherapy-induced\\xa0... A subgroup analysis of 4,405 patients in a large systematic review revealed that current smokers had a significantly higher risk of recurrence compared with\\xa0... Jun 23, 2022 ... Radiation therapy may be given after surgery to reduce the risk of local recurrence for high-risk patients. Targeted drugs can be useful in\\xa0...', 'sources': ['https://www.cancer.gov/news-events/cancer-currents-blog/2023/cancer-survivors-alcohol-drinking-common', 'https://www.cancer.org/content/dam/cancer-org/research/cancer-facts-and-statistics/colorectal-cancer-facts-and-figures/colorectal-cancer-facts-and-figures-2020-2022.pdf', 'https://www.ncbi.nlm.nih.gov/books/NBK553111/', 'https://www.who.int/news-room/fact-sheets/detail/colorectal-cancer', 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5509538/', 'https://www.nature.com/articles/s41572-020-00224-3', 'https://www.nih.gov/about-nih/what-we-do/nih-almanac/national-cancer-institute-nci', 'https://www.nature.com/articles/s41571-023-00776-9', 'https://uroweb.org/guidelines/non-muscle-invasive-bladder-cancer/chapter/disease-management', 'https://acsjournals.onlinelibrary.wiley.com/doi/10.3322/caac.21731']}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: The search results suggest that patients with a history of smoking may experience a higher incidence of cancer recurrence after completing chemotherapy due to various factors. Smoking can increase the risk of cancer recurrence by damaging DNA, reducing the effectiveness of chemotherapy, and increasing the risk of secondary cancers.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Google Search Snippets\",\n",
      "  \"action_input\": \"why does smoking increase the risk of cancer recurrence\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'search_results': \"Dec 7, 2013 ... However, former smokers with 20 to less than 34.9 pack-years of exposure had a 22% increased risk of breast cancer recurrence (hazard ratio [HR]\\xa0... May 20, 2022 ... Several studies have also reported a positive association between smoking and BC recurrence [4, 5, 6]. However, this risk is confined to heavy\\xa0... *Among women who had estrogen receptor-positive breast cancers, current smokers did not have an increased risk of late recurrence (5 or more years after\\xa0... Cigarette smoking is associated with an increased risk of biochemical disease recurrence, metastasis, castration-resistant prostate cancer, and mortality\\xa0... Apr 22, 2024 ... It increases the risk of tobacco-related cancers: If you drink and smoke ... How does drinking alcohol affect a person's chances of cancer\\xa0... Jul 14, 2021 ... How does the combination of alcohol and tobacco affect cancer risk? ... breast cancer recurrence and second primary breast cancer?: A\\xa0... May 15, 2024 ... Increases the risk of death, including death from cancer. · Increases the risk for risk of developing other primary cancers that are smoking-\\xa0... Jun 9, 2020 ... But it is not clear whether alcohol use after treatment might increase the risk of these cancers coming back (recurring). In theory, it's\\xa0... Apr 5, 2022 ... ***Higher BMI is associated with a slight increase in the risk of ovarian cancer ... increased risk of local recurrence (51). Death from\\xa0... Jun 21, 2011 ... A link also was found between smoking at the time of prostate cancer diagnosis and aggressive prostate cancer, overall mortality (death) and\\xa0...\", 'sources': ['https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3906992/', 'https://ejbc.kr/DOIx.php?id=10.4048/jbc.2022.25.e23', 'https://www.komen.org/breast-cancer/facts-statistics/research-studies/topics/smoking-and-breast-cancer-survival/', 'https://pubmed.ncbi.nlm.nih.gov/24127391/', 'https://www.mdanderson.org/cancerwise/does-alcohol-cause-cancer.h00-159383523.html', 'https://www.cancer.gov/about-cancer/causes-prevention/risk/alcohol/alcohol-fact-sheet', 'https://www.cdc.gov/tobacco/hcp/patient-care-settings/cancer.html', 'https://www.cancer.org/cancer/risk-prevention/diet-physical-activity/alcohol-use-and-cancer.html', 'https://www.cancer.gov/about-cancer/causes-prevention/risk/obesity/obesity-fact-sheet', 'https://www.hsph.harvard.edu/news/press-releases/smoking-prostate-cancer-kenfield/']}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: The final answer to the original input question is: Smoking increases the risk of cancer recurrence after completing chemotherapy due to its damaging effects on DNA, reducing the effectiveness of chemotherapy, and increasing the risk of secondary cancers.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "from langchain_core.tools import Tool\n",
    "from langchain.agents import initialize_agent\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "import time\n",
    "\n",
    "llm = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\", max_tokens=8192, min_length=1024)\n",
    "\n",
    "class CustomGoogleSearchWrapper:\n",
    "    def __init__(self, k):\n",
    "        self.all_resources = []\n",
    "        self.search = GoogleSearchAPIWrapper()\n",
    "        self.k = k\n",
    "    def top_k_results(self, query):\n",
    "        res = self.search.results(query, self.k)\n",
    "        # only keep the entries in res if \"snippet\" is in it\n",
    "        res = [r for r in res if \"snippet\" in r]\n",
    "        results = \" \".join([r[\"snippet\"] for r in res])\n",
    "        sources = [r[\"link\"] for r in res]\n",
    "        self.all_resources.extend(sources)\n",
    "        return {\"search_results\" : results, \"sources\" : sources}\n",
    "    def get_all_resources(self):\n",
    "        return self.all_resources\n",
    "    def reset_all_resources(self):\n",
    "        self.all_resources = []\n",
    "\n",
    "def search_for_query(query, llm=None, k=10):\n",
    "    if llm == None:\n",
    "        llm = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\", max_tokens=8192)\n",
    "    search_engine = CustomGoogleSearchWrapper(k)\n",
    "    meta_data_search_tool = Tool(\n",
    "        name=\"Google Search Snippets\",\n",
    "        description=\"Search Google for recent results.\",\n",
    "        func=search_engine.top_k_results,\n",
    "    )\n",
    "    tools = [meta_data_search_tool]\n",
    "    agent = initialize_agent(\n",
    "        tools,\n",
    "        llm,\n",
    "        agent=\"chat-zero-shot-react-description\",\n",
    "        verbose=True,\n",
    "        agent_kwargs={\n",
    "            \"max_execution_time\": 3000,\n",
    "            \"llm_prefix\": (\n",
    "                \"Provide a comprehensive analysis of the information gathered, covering all relevant aspects in depth. Then, respond in detail to the query.\"\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    attempt = 0\n",
    "    while attempt < 5:\n",
    "        try:\n",
    "            res = agent.run(f\"{query}. Please provide multiple perspectives and a detailed breakdown.\")\n",
    "            sources = search_engine.get_all_resources()\n",
    "            # search_engine.reset_all_resources()\n",
    "            return res, sources \n",
    "        except:\n",
    "            print(\"Failed to generate, sleeping for 60 seconds\")\n",
    "            time.sleep(60)\n",
    "            attempt += 1\n",
    "    return None, None\n",
    "\n",
    "result, sources = search_for_query(\"Why do patients with a history of smoking report a higher incidence of cancer recurrence after completing chemotherapy?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The final answer to the original input question is: Smoking increases the risk of cancer recurrence after completing chemotherapy due to its damaging effects on DNA, reducing the effectiveness of chemotherapy, and increasing the risk of secondary cancers.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_df = pd.read_csv(\"data/fineweb_instructions.csv\")\n",
    "instruction_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "search_for_all_queries() missing 1 required positional argument: 'original_query'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m             completed_df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mlen\u001b[39m(completed_df) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m [all_contexts[i], all_instructions[i], res, sources, original_query]\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m completed_df\n\u001b[0;32m---> 20\u001b[0m completed_df \u001b[38;5;241m=\u001b[39m \u001b[43msearch_for_all_queries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhy do patients with a history of smoking report a higher incidence of cancer recurrence after completing chemotherapy?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: search_for_all_queries() missing 1 required positional argument: 'original_query'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def search_for_all_queries(instruction_df, original_query):\n",
    "    llm = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n",
    "    completed_df = pd.DataFrame(columns = [\"original_context\", \"instruction\", \"response\", \"sources\", \"original_query\"])\n",
    "    all_contexts = list(instruction_df[\"text\"])\n",
    "    all_instructions = list(instruction_df[\"instruction\"])\n",
    "    for i in tqdm(range(len(instruction_df))):\n",
    "        res, sources = search_for_query(all_instructions[i], llm)\n",
    "        if res != None and len(res) >= 500 and \"action_input\" not in res:\n",
    "            if \"The final answer to the question is:\" in res and res.index(\"The final answer to the question is:\") == 0:\n",
    "                res = res[len(\"The final answer to the question is:\"):].strip()\n",
    "            if \"The final answer to the original input question is:\" in res and res.index(\"The final answer to the original input question is:\") == 0:\n",
    "                res = res[len(\"The final answer to the original input question is:\"):].strip()\n",
    "            if \"The final answer to the original input question.\" in res and res.index(\"The final answer to the original input question.\") == 0:\n",
    "                res = res[len(\"The final answer to the original input question.\"):].strip()\n",
    "            completed_df.loc[len(completed_df) + 1] = [all_contexts[i], all_instructions[i], res, sources, original_query]\n",
    "    return completed_df\n",
    "\n",
    "\n",
    "completed_df = search_for_all_queries(instruction_df[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_context</th>\n",
       "      <th>instruction</th>\n",
       "      <th>response</th>\n",
       "      <th>sources</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In an increasingly complex financial world, eq...</td>\n",
       "      <td>How can a parent encourage their 12-year-old t...</td>\n",
       "      <td>To encourage a 12-year-old to save for a long-...</td>\n",
       "      <td>[https://529.wa.gov/, https://www.mefa.org/blo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In an increasingly complex financial world, eq...</td>\n",
       "      <td>As a financial literacy expert, how can you he...</td>\n",
       "      <td>To help a young adult develop a savings plan t...</td>\n",
       "      <td>[https://www.nerdwallet.com/article/mortgages/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In an increasingly complex financial world, eq...</td>\n",
       "      <td>How can a parent encourage their 12-year-old c...</td>\n",
       "      <td></td>\n",
       "      <td>[https://www.ramseysolutions.com/relationships...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is Decentralized Finance (DeFi)? A Compre...</td>\n",
       "      <td>How might the development of decentralized aut...</td>\n",
       "      <td>The development of decentralized autonomous or...</td>\n",
       "      <td>[https://home.treasury.gov/system/files/136/De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is Decentralized Finance (DeFi)? A Compre...</td>\n",
       "      <td>Can you compare the economic impact of a decen...</td>\n",
       "      <td>The economic impact of a decentralized financi...</td>\n",
       "      <td>[https://www.fsb.org/wp-content/uploads/P06061...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is Decentralized Finance (DeFi)? A Compre...</td>\n",
       "      <td>How might the development of decentralized aut...</td>\n",
       "      <td>The development of decentralized autonomous or...</td>\n",
       "      <td>[https://home.treasury.gov/system/files/136/De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Running a successful business depends on how w...</td>\n",
       "      <td>How would a financial analyst recommend adjust...</td>\n",
       "      <td>The financial analyst would recommend adjustin...</td>\n",
       "      <td>[https://disclosure.spglobal.com/en/regulatory...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Running a successful business depends on how w...</td>\n",
       "      <td>A company is experiencing a cash crunch due to...</td>\n",
       "      <td>To improve its cash flow situation, the compan...</td>\n",
       "      <td>[https://upflow.io/blog/ar-collections/account...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Running a successful business depends on how w...</td>\n",
       "      <td>What are the potential consequences of not pro...</td>\n",
       "      <td>The consequences of not properly managing a co...</td>\n",
       "      <td>[https://www.linkedin.com/advice/0/what-risks-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What is Business Credit?\\nBusiness credit refe...</td>\n",
       "      <td>Can a small business owner with a limited cred...</td>\n",
       "      <td>Yes, a small business owner with a limited cre...</td>\n",
       "      <td>[https://www.ffiec.gov/hmda/pdf/2021Guide.pdf,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What is Business Credit?\\nBusiness credit refe...</td>\n",
       "      <td>Can a small business owner use business credit...</td>\n",
       "      <td>The final answer to the original input questio...</td>\n",
       "      <td>[https://www.sba.gov/business-guide/plan-your-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What is Business Credit?\\nBusiness credit refe...</td>\n",
       "      <td>What are some common mistakes that small busin...</td>\n",
       "      <td>Establishing a strong business credit profile ...</td>\n",
       "      <td>[https://www.quora.com/What-is-the-worst-mista...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Basic Guide To Business Process And Busine...</td>\n",
       "      <td>What strategy can a company use to improve its...</td>\n",
       "      <td>To improve customer satisfaction score by 20% ...</td>\n",
       "      <td>[https://kaizo.com/blog/improve-customer-satis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The Basic Guide To Business Process And Busine...</td>\n",
       "      <td>How can a company effectively combine its mark...</td>\n",
       "      <td>To effectively combine marketing and sales tea...</td>\n",
       "      <td>[https://community.hubspot.com/t5/Tips-Tricks-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The Basic Guide To Business Process And Busine...</td>\n",
       "      <td>Can you recommend a strategy to streamline a c...</td>\n",
       "      <td>To streamline a company's customer service ope...</td>\n",
       "      <td>[https://www.startingpoint.ai/post/15-ways-str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What is Finance?\\nIs it worth it? This is the ...</td>\n",
       "      <td>What is the optimal mix of investments for a p...</td>\n",
       "      <td>The optimal mix of investments for a pension f...</td>\n",
       "      <td>[https://www.schwab.com/learn/story/structurin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What is Finance?\\nIs it worth it? This is the ...</td>\n",
       "      <td>Can a company's valuation multiples justify a ...</td>\n",
       "      <td>To determine if a company's valuation multiple...</td>\n",
       "      <td>[https://www.fe.training/free-resources/ma/val...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What is Finance?\\nIs it worth it? This is the ...</td>\n",
       "      <td>What strategic factors should a company consid...</td>\n",
       "      <td>The strategic factors to consider when decidin...</td>\n",
       "      <td>[https://mconsultingprep.com/market-entry-fram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>When it comes to pursuing a career in the busi...</td>\n",
       "      <td>What are the key factors that influence an org...</td>\n",
       "      <td>The key factors that influence an organization...</td>\n",
       "      <td>[https://www.weforum.org/publications/the-futu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>When it comes to pursuing a career in the busi...</td>\n",
       "      <td>What are the key factors that an employer cons...</td>\n",
       "      <td>The key factors that an employer considers whe...</td>\n",
       "      <td>[https://www.opm.gov/policy-data-oversight/cla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     original_context  \\\n",
       "1   In an increasingly complex financial world, eq...   \n",
       "2   In an increasingly complex financial world, eq...   \n",
       "3   In an increasingly complex financial world, eq...   \n",
       "4   What is Decentralized Finance (DeFi)? A Compre...   \n",
       "5   What is Decentralized Finance (DeFi)? A Compre...   \n",
       "6   What is Decentralized Finance (DeFi)? A Compre...   \n",
       "7   Running a successful business depends on how w...   \n",
       "8   Running a successful business depends on how w...   \n",
       "9   Running a successful business depends on how w...   \n",
       "10  What is Business Credit?\\nBusiness credit refe...   \n",
       "11  What is Business Credit?\\nBusiness credit refe...   \n",
       "12  What is Business Credit?\\nBusiness credit refe...   \n",
       "13  The Basic Guide To Business Process And Busine...   \n",
       "14  The Basic Guide To Business Process And Busine...   \n",
       "15  The Basic Guide To Business Process And Busine...   \n",
       "16  What is Finance?\\nIs it worth it? This is the ...   \n",
       "17  What is Finance?\\nIs it worth it? This is the ...   \n",
       "18  What is Finance?\\nIs it worth it? This is the ...   \n",
       "19  When it comes to pursuing a career in the busi...   \n",
       "20  When it comes to pursuing a career in the busi...   \n",
       "\n",
       "                                          instruction  \\\n",
       "1   How can a parent encourage their 12-year-old t...   \n",
       "2   As a financial literacy expert, how can you he...   \n",
       "3   How can a parent encourage their 12-year-old c...   \n",
       "4   How might the development of decentralized aut...   \n",
       "5   Can you compare the economic impact of a decen...   \n",
       "6   How might the development of decentralized aut...   \n",
       "7   How would a financial analyst recommend adjust...   \n",
       "8   A company is experiencing a cash crunch due to...   \n",
       "9   What are the potential consequences of not pro...   \n",
       "10  Can a small business owner with a limited cred...   \n",
       "11  Can a small business owner use business credit...   \n",
       "12  What are some common mistakes that small busin...   \n",
       "13  What strategy can a company use to improve its...   \n",
       "14  How can a company effectively combine its mark...   \n",
       "15  Can you recommend a strategy to streamline a c...   \n",
       "16  What is the optimal mix of investments for a p...   \n",
       "17  Can a company's valuation multiples justify a ...   \n",
       "18  What strategic factors should a company consid...   \n",
       "19  What are the key factors that influence an org...   \n",
       "20  What are the key factors that an employer cons...   \n",
       "\n",
       "                                             response  \\\n",
       "1   To encourage a 12-year-old to save for a long-...   \n",
       "2   To help a young adult develop a savings plan t...   \n",
       "3                                                       \n",
       "4   The development of decentralized autonomous or...   \n",
       "5   The economic impact of a decentralized financi...   \n",
       "6   The development of decentralized autonomous or...   \n",
       "7   The financial analyst would recommend adjustin...   \n",
       "8   To improve its cash flow situation, the compan...   \n",
       "9   The consequences of not properly managing a co...   \n",
       "10  Yes, a small business owner with a limited cre...   \n",
       "11  The final answer to the original input questio...   \n",
       "12  Establishing a strong business credit profile ...   \n",
       "13  To improve customer satisfaction score by 20% ...   \n",
       "14  To effectively combine marketing and sales tea...   \n",
       "15  To streamline a company's customer service ope...   \n",
       "16  The optimal mix of investments for a pension f...   \n",
       "17  To determine if a company's valuation multiple...   \n",
       "18  The strategic factors to consider when decidin...   \n",
       "19  The key factors that influence an organization...   \n",
       "20  The key factors that an employer considers whe...   \n",
       "\n",
       "                                              sources  \n",
       "1   [https://529.wa.gov/, https://www.mefa.org/blo...  \n",
       "2   [https://www.nerdwallet.com/article/mortgages/...  \n",
       "3   [https://www.ramseysolutions.com/relationships...  \n",
       "4   [https://home.treasury.gov/system/files/136/De...  \n",
       "5   [https://www.fsb.org/wp-content/uploads/P06061...  \n",
       "6   [https://home.treasury.gov/system/files/136/De...  \n",
       "7   [https://disclosure.spglobal.com/en/regulatory...  \n",
       "8   [https://upflow.io/blog/ar-collections/account...  \n",
       "9   [https://www.linkedin.com/advice/0/what-risks-...  \n",
       "10  [https://www.ffiec.gov/hmda/pdf/2021Guide.pdf,...  \n",
       "11  [https://www.sba.gov/business-guide/plan-your-...  \n",
       "12  [https://www.quora.com/What-is-the-worst-mista...  \n",
       "13  [https://kaizo.com/blog/improve-customer-satis...  \n",
       "14  [https://community.hubspot.com/t5/Tips-Tricks-...  \n",
       "15  [https://www.startingpoint.ai/post/15-ways-str...  \n",
       "16  [https://www.schwab.com/learn/story/structurin...  \n",
       "17  [https://www.fe.training/free-resources/ma/val...  \n",
       "18  [https://mconsultingprep.com/market-entry-fram...  \n",
       "19  [https://www.weforum.org/publications/the-futu...  \n",
       "20  [https://www.opm.gov/policy-data-oversight/cla...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Upload of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/orbina/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edeb143daaf8455985cc6487fabd7665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ad511ef64746b1988ec986915ad081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd118c5f1b14c6b8bca8e676c5a009f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f8c4dc02584dbc982cc6f58299b4a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/442 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/bs82/NexaAIHackathonDemo/commit/66878660b34181e08336e34c7f7d99a52ea566a8', commit_message='Upload dataset', commit_description='', oid='66878660b34181e08336e34c7f7d99a52ea566a8', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "login(os.environ[\"HUGGINGFACE_TOKEN\"])\n",
    "\n",
    "folder_path = \"demo_result\"\n",
    "\n",
    "data_files = {\n",
    "    \"train\": [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "}\n",
    "\n",
    "dataset_dict = load_dataset(\"csv\", data_files=data_files)\n",
    "dataset_dict.push_to_hub(\"bs82/NexaAIHackathonDemo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nexa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
