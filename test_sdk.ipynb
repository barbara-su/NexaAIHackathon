{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook Records Development Process of Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare all env  keys\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open('keys.json', 'r') as file:\n",
    "    api_keys = json.load(file)\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = api_keys[\"GROQ_API_KEY\"]\n",
    "os.environ[\"JINA_API_KEY\"] = api_keys[\"JINA_API_KEY\"]\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = api_keys[\"GOOGLE_CSE_ID\"]\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_keys[\"GOOGLE_API_KEY\"]\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = api_keys[\"HUGGINGFACE_TOKEN\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"*rubs against leg* Oh, hello there human friend! *purrs* I'm so glad you're talking to me. You know, I've been thinking... it's about time you refilled my food dish. I mean, a cat's gotta eat, right? *bats at ankles* And maybe you could scratch behind my ears a bit? That feels so good. *purrs some more* Oh, and by the way, I think I saw a fly buzzing around the room. You know, the one with the annoying buzzing noise? Yeah, that one. I think I could catch it if you just give me a little encouragement. *bats at air*\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from groq import Groq\n",
    "import time \n",
    "from langchain_chroma import Chroma\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from langchain_community.embeddings import JinaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from typing import List\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "from langchain_core.tools import Tool\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain_groq import ChatGroq\n",
    "import json\n",
    "\n",
    "with open('keys.json', 'r') as file:\n",
    "    api_keys = json.load(file)\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = api_keys[\"GROQ_API_KEY\"]\n",
    "os.environ[\"JINA_API_KEY\"] = api_keys[\"JINA_API_KEY\"]\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = api_keys[\"GOOGLE_CSE_ID\"]\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_keys[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "def call_groq(raw_prompt, temperature=0):\n",
    "    client = Groq(\n",
    "        api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "    )\n",
    "    attempt = 0\n",
    "    while attempt < 5:\n",
    "        try:\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                temperature=temperature,\n",
    "                max_tokens=8192,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"you are a helpful assistant.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": raw_prompt,\n",
    "                    }\n",
    "                ],\n",
    "                model=\"llama3-8b-8192\",\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content\n",
    "        except:\n",
    "            print(\"Rate limite exceeded, sleeping for 20 seconds\")\n",
    "            time.sleep(20)\n",
    "            attempt += 1\n",
    "    print(\"Failed to generate!\")\n",
    "    return None\n",
    "call_groq(\"Pretend you are a cat. What will you say?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d7e96fd1ab4be7953fd86242996fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e7de8ee94c4918b53040a1593377fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Stream the dataset\n",
    "dataset = load_dataset(\"HuggingFaceFW/fineweb-edu\", data_files=\"data/CC-MAIN-2024-10/*.parquet\", split='train', streaming=True)\n",
    "\n",
    "# get first 10000 rows of dataset\n",
    "def get_first_n_rows(dataset, n=10000):\n",
    "    first_n_rows = []\n",
    "    for i, row in enumerate(dataset):\n",
    "        first_n_rows.append(row)\n",
    "        if i == n - 1:\n",
    "            break \n",
    "    return first_n_rows\n",
    "n_rows = get_first_n_rows(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the text, and save it to local dataset\n",
    "import pandas as pd\n",
    "fineweb_edu_2024_10_subset = pd.DataFrame(data={\"text\" : [row[\"text\"] for row in n_rows]})\n",
    "fineweb_edu_2024_10_subset.to_csv(\"data/fineweb_edu_2024_10_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>– Computer viruses are parasitic programs whic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For those unfamiliar with Cornish, it is class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Our cultural identity: Experience the culture ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“The more you empower kids, the more they can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mixed Progress Against Cancers in Teens, Young...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rhetorical analysis is not for the faint of he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sport plays an important role in the education...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>World's first 3D keyhole surgery performed at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Lodge Pole Pine Christmas tree is a native...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>After the famous earthquake of 1755 that destr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  – Computer viruses are parasitic programs whic...\n",
       "1  For those unfamiliar with Cornish, it is class...\n",
       "2  Our cultural identity: Experience the culture ...\n",
       "3  “The more you empower kids, the more they can ...\n",
       "4  Mixed Progress Against Cancers in Teens, Young...\n",
       "5  Rhetorical analysis is not for the faint of he...\n",
       "6  Sport plays an important role in the education...\n",
       "7  World's first 3D keyhole surgery performed at ...\n",
       "8  The Lodge Pole Pine Christmas tree is a native...\n",
       "9  After the famous earthquake of 1755 that destr..."
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fineweb_edu_2024_10_subset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download NexaAI model\n",
    "from transformers import AutoTokenizer, GemmaForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"NexaAIDev/Octopus-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = GemmaForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.save_pretrained(\"octopus-v2\")\n",
    "tokenizer.save_pretrained(\"octopus-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0986d1693e408986297d69fff369dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'What is the capital of China?\\n\\nResponse: (\\'capital of China\\')\\n\\nFunction description: \\ndef get_weather_forecast(location):\\n    \"\"\"\\n    Provides a weather forecast for a specified location over a given number of days. Each day\\'s forecast includes a brief description of the expected weather conditions.\\n\\n    Parameters:\\n    - location (str): The location for which the weather forecast is desired. Can be a city name, ZIP code, or other location identifiers.\\n\\n    Returns'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the already-saved model\n",
    "from transformers import AutoTokenizer, GemmaForCausalLM, pipeline\n",
    "import torch\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "model_dir = \"octopus-v2\"\n",
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = GemmaForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, device_map=device\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=100\n",
    ")\n",
    "\n",
    "octopus_llm = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Call the function run() if I ask you about athletic question. Call the function eat() if I ask you about food question. Question: who is a runner?\\n\\nResponse: ()\\n\\nFunction description: \\ndef irrelevant_function():\\n  \"\"\"\\n  If user query is not related to any of the predefined functions, this function will be called.\\n  \\n  Args:\\n  \\n  Returns:\\n  \"\"\"\\n\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it seems that the octopus llm can only create functions. So it did not work\n",
    "octopus_llm(\"Call the function run() if I ask you about athletic question. Call the function eat() if I ask you about food question. Question: who is a runner?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Low-latency Large Language Models (LLMs) are a crucial development in the field of natural language processing (NLP) and artificial intelligence (AI). Here's why:\\n\\n**What are Low-Latency LLMs?**\\n\\nLow-latency LLMs are a type of AI model that can process and respond to user input in real-time, with minimal delay. Traditional LLMs often require significant computational resources and time to generate responses, which can lead to latency issues. Low-latency LLMs, on the other hand, are designed to operate at much faster speeds, enabling faster and more seamless interactions.\\n\\n**Importance of Low-Latency LLMs:**\\n\\n1. **Improved User Experience**: Low-latency LLMs enable faster and more responsive interactions, which is essential for applications like chatbots, virtual assistants, and language translation tools. Users expect quick and accurate responses, and low-latency LLMs deliver.\\n2. **Enhanced Conversational Flow**: With low-latency LLMs, conversations can flow more naturally, allowing users to engage in more productive and meaningful interactions. This is particularly important for applications like customer support, where timely responses are critical.\\n3. **Increased Productivity**: Low-latency LLMs can automate routine tasks, freeing up human resources for more complex and creative work. For example, in customer service, low-latency LLMs can handle simple queries, allowing human representatives to focus on more complex issues.\\n4. **Competitive Advantage**: Companies that adopt low-latency LLMs can gain a competitive edge in their respective markets. By providing faster and more accurate responses, they can differentiate themselves from competitors and build stronger relationships with customers.\\n5. **Enabling New Applications**: Low-latency LLMs open up new possibilities for applications like:\\n\\t* Real-time language translation\\n\\t* Virtual event moderation\\n\\t* Live chat support\\n\\t* Personalized content generation\\n\\t* Intelligent tutoring systems\\n6. **Advancements in Edge AI**: Low-latency LLMs are a key enabler of edge AI, which involves processing data and running AI models on edge devices, such as smartphones, smart home devices, or IoT sensors. This reduces latency and improves responsiveness, making edge AI more practical and effective.\\n7. **Improved Model Accuracy**: Low-latency LLMs often require more efficient model architectures and training techniques, which can lead to improved model accuracy and reduced overfitting.\\n\\nIn summary, low-latency LLMs are essential for creating seamless, responsive, and productive interactions between humans and AI systems. Their importance extends beyond just improving user experience, as they also enable new applications, drive innovation, and provide a competitive advantage in the market.\", response_metadata={'token_usage': {'completion_tokens': 553, 'prompt_tokens': 32, 'total_tokens': 585, 'completion_time': 0.737333333, 'prompt_time': 0.018012384, 'queue_time': None, 'total_time': 0.7553457170000001}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f66ccb39ec', 'finish_reason': 'stop', 'logprobs': None}, id='run-4531c070-d533-4a14-9b66-eb76197d59e9-0', usage_metadata={'input_tokens': 32, 'output_tokens': 553, 'total_tokens': 585})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# switch to groq instead...\n",
    "import os \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(temperature=0, model_name=\"llama-3.1-8b-instant\")\n",
    "\n",
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke({\"text\": \"Explain the importance of low latency LLMs.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Matching (Does not work!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a topic, generate 30 keywords by prompting LLM\n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "def call_groq(raw_prompt):\n",
    "    client = Groq(\n",
    "        api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "    )\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"you are a helpful assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": raw_prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"mixtral-8x7b-32768\",\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "def parse_keyword(raw_result):\n",
    "    try:\n",
    "        keywords = raw_result[raw_result.index(\"Keywords:\") + len(\"Keywords:\"):].strip()\n",
    "        if keywords[-1] == \".\":\n",
    "            keywords = keywords[:-1]\n",
    "        keywords = [keyword.strip().lower() for keyword in keywords.split(\",\")]\n",
    "        return keywords\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def generate_keywords(topic, k, max_attempt = 5):    \n",
    "    raw_prompt = f\"Please list exactly {k} keywords related to the to the topic {topic}. Just list the words. You should not include index or explanation.\"\n",
    "    raw_prompt += \"Begin your response with 'Keywords:', followed by <keyword_1>, <keyword_2>, ...\"\n",
    "    raw_result = call_groq(raw_prompt)\n",
    "    attempt = 0\n",
    "    while attempt < max_attempt:\n",
    "        result = parse_keyword(raw_result)\n",
    "        if result != None:\n",
    "            return set([topic.lower()] + result)\n",
    "        else:\n",
    "            attempt += 1\n",
    "    return None\n",
    "\n",
    "res = generate_keywords(\"finance\", 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def preprocess(doc):\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "    return tokens\n",
    "\n",
    "def is_topic_document(doc, keywords, threshold = 15):\n",
    "    tokens = preprocess(doc)\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        if token in keywords:\n",
    "            count += 1\n",
    "        if count == threshold:\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:45<00:00, 220.53it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "true_docs = []\n",
    "texts = pd.read_csv(\"data/fineweb_edu_2024_10_subset.csv\")[\"text\"]\n",
    "\n",
    "for text in tqdm(texts):\n",
    "    if is_topic_document(text, res):\n",
    "        true_docs.append(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In world practice, there are two classical types of pension systems based on the principle of financing: pay-as-you-go and funded. In the pay-as-you-go pension system, payments to pensioners are made at the expense of the current income of workers (current tax revenues to the budget). In the funded system, the working generation pays contributions that are not spent on payments to the elderly but accumulated, invested and, together with the investment return, subsequently used to provide pensions to those who have been saving.\\nWe can particularly highlight the notional-defined contribution pension system used in some countries, it combines elements of the pay-as-you-go and funded types of pension systems. Pension entitlement is earned by the participant's contributions to the pension system. At some point, prospective retirees enter into a deferred retirement annuity contract with a life insurance company.\\n“A deferred insurance annuity is an insurance agreement under which the insured person receives regular payments from a certain moment in the future. The term for these payments can be established by the contract, otherwise payments are made during the life of the insured person. Such contracts are usually concluded with the aim of maintaining income levels after the end of employment. The insurance company assumes the risk of longevity, that is, if a person lives longer than the average life, his income in total may exceed the amount that can be obtained from other investments. At the same time, if a person dies earlier than average, he or she loses some of the income. Thus, long-livers receive more income from contracts of customers with a shorter lifespan. The corresponding additional income of centenarians is called mortality credit,” explains ACRA.\\nThis type of insurance is common in the United States. In general, all able-bodied US citizens can independently form their pension. There is a 401 (k) plan, a special account to which the employer transfers funds. This allows the employee to transfer a percentage of the salary to a tax-free account. More precisely, the tax is postponed until the time of retirement. The Americans save on taxes this way while making savings for the future. 401 (k) payments are usually 6% of pre-tax salary.\\nSIMPLE IRA is the second most popular pension plan. It is also called the IRA Employee Incentive Savings Account. Such a plan is most often offered by small private companies: the employer pays 1% -3% of the salary before taxes.\\nSEP IRA is a type of retirement insurance for self-employed people. The main distinguishing feature of IRA SEP is its high contribution limits. You can deposit up to 25% of your gross annual salary or 20% into your retirement account.\\nSocial Security is a federal financial assistance program for the US residents and their dependent family members.\\nThe US Treasury completed the creation of requirements for deferred life retirement annuity contracts or QLACs in the summer of 2014. This document provides an exception to taxation rules and additional requirements for insurers. For example, a person can pay $20,000 from their retirement savings at age 60 to purchase longevity insurance, which will receive $11,803 a year from age 85 until death. In this example, we can see that if a person lives to be 95, he will receive $118,030 for his $20,000. This is a rate of return that far exceeds the available profit from market rates.\\nThe economic reason for high returns with low risk is that policyholders waive any claims for the initial investment. If a person dies before age 85 (the maximum age until which QLAC funds can be deferred), the insurance company pays nothing to his heirs and is not liable for his obligations to creditors.\\nAnother reason for the high profitability is the rather long investment period, due to the delay in payment, which allows the insurance company to use less conservative investment policies. Besides, this retirement plan keeps the life insurance company’s client in the lower tax category, which results in good cost savings with a high investment return.\\nMany people buy such an annuity because it allows them to have financial protection in old age for a fairly small amount. Another benefit of QLAC is that it allows spouses create a joint deferred annuity.\\nPhotos are from open sources.\""
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_llm_response(raw_result, header):\n",
    "    try:\n",
    "        res = raw_result[raw_result.index(header) + len(header):].strip()\n",
    "        if res[-1] == \".\":\n",
    "            res = res[:-1]\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def is_topic_document_llm(doc, topic, attempt = 5):\n",
    "    raw_prompt = f\"Here is a text: {doc}\"\n",
    "    raw_prompt += \"Read this text carefully. Then determine whether this text belongs to the topic {topic}.\"\n",
    "    raw_prompt += \"Begin your answer by 'Answer:', followed by true or fasle. Do not include reasoning or words other than true or fasle.\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accounting',\n",
       " 'assets',\n",
       " 'audit',\n",
       " 'bank',\n",
       " 'bonds',\n",
       " 'budget',\n",
       " 'budgeting',\n",
       " 'credit',\n",
       " 'debit',\n",
       " 'dividends',\n",
       " 'economy',\n",
       " 'expenses',\n",
       " 'finance',\n",
       " 'financial advisor',\n",
       " 'financial planning',\n",
       " 'financial statements',\n",
       " 'income',\n",
       " 'inflation',\n",
       " 'insurance',\n",
       " 'interest',\n",
       " 'investing',\n",
       " 'liabilities',\n",
       " 'loan',\n",
       " 'mortgage',\n",
       " 'pension',\n",
       " 'portfolio',\n",
       " 'retirement',\n",
       " 'savings',\n",
       " 'stocks',\n",
       " 'taxes'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Based Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplistic version of using BM25 retriever\n",
    "from langchain.retrievers import BM25Retriever\n",
    "import pandas as pd\n",
    "import os\n",
    "from groq import Groq\n",
    "import time \n",
    "\n",
    "def call_groq(raw_prompt, temperature=0):\n",
    "    client = Groq(\n",
    "        api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "    )\n",
    "    attempt = 0\n",
    "    while attempt < 5:\n",
    "        try:\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                temperature=temperature,\n",
    "                max_tokens=8192,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"you are a helpful assistant.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": raw_prompt,\n",
    "                    }\n",
    "                ],\n",
    "                model=\"llama3-8b-8192\",\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content\n",
    "        except:\n",
    "            print(\"Rate limite exceeded, sleeping for 5 seconds\")\n",
    "            time.sleep(5)\n",
    "            attempt += 1\n",
    "    print(\"Failed to generate!\")\n",
    "    return None\n",
    "\n",
    "def filter_text_by_topic(texts, topic, k=100):\n",
    "    description = call_groq(f\"Elaborate on this: {topic}.\")\n",
    "    bm25_retriever = BM25Retriever.from_texts(texts)\n",
    "    bm25_retriever.k = k\n",
    "    docs = bm25_retriever.get_relevant_documents(description)\n",
    "    docs = [doc.page_content for doc in docs]\n",
    "    return docs\n",
    "\n",
    "texts = pd.read_csv(\"data/fineweb_edu_2024_10_subset.csv\")[\"text\"]\n",
    "# texts = filter_text_by_topic(texts, \"finance\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15068"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import os\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import JinaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class MyEmbeddings:\n",
    "    def __init__(self):\n",
    "        self.embedding_func = JinaEmbeddings(model_name=\"jina-embeddings-v2-base-en\")\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        embedding_func = JinaEmbeddings(model_name=\"jina-embeddings-v2-base-en\")\n",
    "        batch_size = 2048\n",
    "        embeddings = []\n",
    "        n = len(texts)\n",
    "        for i in tqdm(range(0, n, batch_size)):\n",
    "            page_contents = []\n",
    "            for j in range(i, i + batch_size):\n",
    "                if j >= n:\n",
    "                    break\n",
    "                page_contents.append(texts[j])\n",
    "            if len(page_contents) > 0:\n",
    "                embeddings.extend(embedding_func.embed_documents(page_contents))\n",
    "        return embeddings \n",
    "\n",
    "\n",
    "embeddings = MyEmbeddings()\n",
    "\n",
    "# Initialize the text splitter with the desired chunk size and overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=5000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    separators=[\n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \" \",\n",
    "        \".\",\n",
    "        \",\",\n",
    "        \"\\u200b\",  # Zero-width space\n",
    "        \"\\uff0c\",  # Fullwidth comma\n",
    "        \"\\u3001\",  # Ideographic comma\n",
    "        \"\\uff0e\",  # Fullwidth full stop\n",
    "        \"\\u3002\",  # Ideographic full stop\n",
    "        \"\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "texts = list(pd.read_csv(\"data/fineweb_edu_2024_10_subset.csv\")[\"text\"])\n",
    "\n",
    "documents = text_splitter.create_documents(texts = texts)\n",
    "\n",
    "len(documents)\n",
    "# splitted_text = text_splitter.split_text(texts)\n",
    "# embeddings = embed_by_batch(texts, embedding_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [03:35<00:00, 26.91s/it]\n"
     ]
    }
   ],
   "source": [
    "# create vector store\n",
    "\n",
    "persist_directory = 'fineweb_db'\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=documents,\n",
    "                                 embedding=embeddings,\n",
    "                                 persist_directory=persist_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/orbina/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# load the database\n",
    "from langchain_chroma import Chroma\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from langchain_community.embeddings import JinaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from typing import List\n",
    "\n",
    "persist_directory = 'fineweb_db'\n",
    "class CustomJinaEmbeddings:\n",
    "    def __init__(self):\n",
    "        self.embedding_func = JinaEmbeddings(model_name=\"jina-embeddings-v2-base-en\")\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        embedding_func = JinaEmbeddings(model_name=\"jina-embeddings-v2-base-en\")\n",
    "        batch_size = 2048\n",
    "        embeddings = []\n",
    "        n = len(texts)\n",
    "        for i in tqdm(range(0, n, batch_size)):\n",
    "            page_contents = []\n",
    "            for j in range(i, i + batch_size):\n",
    "                if j >= n:\n",
    "                    break\n",
    "                page_contents.append(texts[j])\n",
    "            if len(page_contents) > 0:\n",
    "                embeddings.extend(embedding_func.embed_documents(page_contents))\n",
    "        return embeddings \n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.embedding_func.embed_query(text)\n",
    "\n",
    "\n",
    "embeddings = CustomJinaEmbeddings()\n",
    "\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 100})\n",
    "\n",
    "topic = \"Finance\"\n",
    "description = call_groq(f\"Elaborate on this: {topic}.\")\n",
    "docs = retriever.get_relevant_documents(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:52<00:00,  1.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "splitted_texts = [doc.page_content for doc in docs]\n",
    "def create_groq_evaluate_prompt(query, context):\n",
    "    prompt = f\"Here is a context I want you to consider: {context}\\n\"\n",
    "    prompt += f\"Here is a query: {query}\\n\" \n",
    "    prompt += \"\"\"Write a brief analysis of whether the context is related to the query. Then conclude by writing \"Result:\", followed by \"yes\" or \"no\".\"\"\"\n",
    "    return prompt\n",
    "def parse_res(header, response):\n",
    "    try:\n",
    "        res = response[response.index(header) + len(header):].strip()\n",
    "        return res \n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def evaluate_context(query, context):\n",
    "    evaluate_prompt = create_groq_evaluate_prompt(query, context)\n",
    "    result = parse_res(\"Result:\", call_groq(evaluate_prompt))\n",
    "    if result == \"yes\":\n",
    "        return True \n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "import threading\n",
    "def multiprocess(func, iterable, n_workers):\n",
    "    threads = []\n",
    "    result  = {}\n",
    "    def worker():\n",
    "        while True:\n",
    "            xs = next(iterable, None)\n",
    "            if xs == None:\n",
    "                break\n",
    "            result[xs] = func(*xs)\n",
    "    for _ in range(n_workers):\n",
    "        threads.append(threading.Thread(target = worker))\n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    return result\n",
    "\n",
    "\n",
    "input_contexts = [(\"Finance\", splitted_texts[i]) for i in range(len(splitted_texts))]\n",
    "\n",
    "result = []\n",
    "for i in tqdm(range(len(splitted_texts))):\n",
    "    result.append(evaluate_context(\"Finance\", splitted_texts[i]))\n",
    "# result = multiprocess(evaluate_context, iter(input_contexts), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remaining_texts = []\n",
    "# for r in result:\n",
    "#     if result[r] == True:\n",
    "#         remaining_texts.append(r)\n",
    "# len(remaining_texts)\n",
    "remaining_texts = []\n",
    "for i in range(len(result)):\n",
    "    if result[i] == True:\n",
    "        remaining_texts.append(splitted_texts[i])\n",
    "len(remaining_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate instructions through GenQA process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_genq_prompt(context, n1=20, n2=20):\n",
    "    prompt = \"Here is the context you need to consider: \"\n",
    "    prompt += f\"{context} \\n\"\n",
    "    prompt += f\"Now, list {n1} topics that you can answer questions about in relation to this context. Select a random topic from this list and specify it.\\n\"\n",
    "    prompt += f\"Then write {n2} subtopics about the selected topic. Select a random subtopic from this list and specify it.\\n\"\n",
    "    prompt += \"Next, write a question that is not directly related to the subtopic but requires expertise in the subtopic and the given context.\"\n",
    "    prompt += \"The name of the subtopic should not appear in the question, and the words in the subtopic should not be used in the question.\"\n",
    "    prompt += \"\"\"Start your questions with \"Question:\". Be creative.\"\"\"\n",
    "    return prompt \n",
    "\n",
    "def parse_res(header, response):\n",
    "    try:\n",
    "        res = response[response.index(header) + len(header):].strip()\n",
    "        return res \n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def genq_by_context(context, n1=20, n2=20, max_attempt=5):\n",
    "    genq_prompt = create_genq_prompt(context, n1, n2)\n",
    "    attempt = 0\n",
    "    while attempt < max_attempt:\n",
    "        response = call_groq(genq_prompt, temperature=0.8)\n",
    "        genq_result = parse_res(\"Question:\", response)\n",
    "        if genq_result != None:\n",
    "            return genq_result \n",
    "        attempt += 1\n",
    "    return None \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/orbina/.cache/huggingface/token\n",
      "Login successful\n",
      "0 texts splitted!\n",
      "1 texts splitted!\n",
      "2 texts splitted!\n",
      "3 texts splitted!\n",
      "4 texts splitted!\n",
      "5 texts splitted!\n",
      "6 texts splitted!\n",
      "7 texts splitted!\n",
      "8 texts splitted!\n",
      "9 texts splitted!\n",
      "10 texts splitted!\n",
      "11 texts splitted!\n",
      "12 texts splitted!\n",
      "13 texts splitted!\n",
      "14 texts splitted!\n",
      "15 texts splitted!\n",
      "16 texts splitted!\n",
      "17 texts splitted!\n",
      "18 texts splitted!\n",
      "19 texts splitted!\n",
      "20 texts splitted!\n",
      "21 texts splitted!\n",
      "22 texts splitted!\n",
      "23 texts splitted!\n",
      "24 texts splitted!\n",
      "25 texts splitted!\n",
      "26 texts splitted!\n",
      "27 texts splitted!\n",
      "28 texts splitted!\n",
      "29 texts splitted!\n",
      "30 texts splitted!\n",
      "31 texts splitted!\n",
      "32 texts splitted!\n",
      "33 texts splitted!\n",
      "34 texts splitted!\n",
      "35 texts splitted!\n",
      "36 texts splitted!\n",
      "37 texts splitted!\n",
      "38 texts splitted!\n",
      "39 texts splitted!\n",
      "40 texts splitted!\n",
      "41 texts splitted!\n",
      "42 texts splitted!\n",
      "43 texts splitted!\n",
      "44 texts splitted!\n",
      "45 texts splitted!\n",
      "46 texts splitted!\n",
      "47 texts splitted!\n",
      "48 texts splitted!\n",
      "49 texts splitted!\n"
     ]
    }
   ],
   "source": [
    "# in case we want to split the text to avoid exceeding max tokens\n",
    "import multiprocessing.dummy as mp\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import login \n",
    "from tqdm import tqdm\n",
    "\n",
    "def split_text_by_punctuation(text, num_pieces):\n",
    "    punctuation = [\".\", \"!\", \"?\"]\n",
    "    best_split = None\n",
    "    min_difference = float(\"inf\")\n",
    "    for punct in punctuation:\n",
    "        sentences = text.split(punct)\n",
    "        if len(sentences) <= num_pieces:\n",
    "            continue\n",
    "        candidates = [\"\"] * num_pieces\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            candidates[i % num_pieces] += sentence + punct\n",
    "        \n",
    "        lengths = [len(candidate) for candidate in candidates]\n",
    "        max_length = max(lengths)\n",
    "        min_length = min(lengths)\n",
    "        difference = max_length - min_length\n",
    "        if difference < min_difference:\n",
    "            min_difference = difference\n",
    "            best_split = candidates\n",
    "    \n",
    "    return best_split\n",
    "\n",
    "def recursive_split(text, max_tokens=4000, num_pieces=2):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "    tokens = tokenizer.encode(text)\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return [text]\n",
    "    split_texts = split_text_by_punctuation(text, num_pieces)\n",
    "    if not split_texts:\n",
    "        return [text]  # If no good split found, return as is\n",
    "    pieces = []\n",
    "    for part in split_texts:\n",
    "        pieces.extend(recursive_split(part, max_tokens, num_pieces + 1))\n",
    "    return pieces\n",
    "\n",
    "login(\"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\")\n",
    "splitted_texts = []\n",
    "count = 0\n",
    "for text in texts:\n",
    "    splitted_texts.extend(recursive_split(text, max_tokens=5000))\n",
    "    print(f\"{count} texts splitted!\")\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limite exceeded, sleeping for 5 seconds\n",
      "Rate limite exceeded, sleeping for 5 seconds\n",
      "Rate limite exceeded, sleeping for 5 seconds\n",
      "Rate limite exceeded, sleeping for 5 seconds\n",
      "Rate limite exceeded, sleeping for 5 seconds\n",
      "Rate limite exceeded, sleeping for 5 seconds\n"
     ]
    }
   ],
   "source": [
    "# multiprocessing generation\n",
    "import pandas as pd\n",
    "import multiprocessing.dummy as mp\n",
    "import time \n",
    "\n",
    "start_time = time.time()\n",
    "def multiprocess_multiple_variables(func, iterable, n_workers):\n",
    "    pool = mp.Pool(processes=n_workers)\n",
    "    result = pool.starmap(func, iterable)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return result\n",
    "\n",
    "def gen_m_q_for_n_context_multiprocess(contexts, m, n1=20, n2=20, max_attempt=5, n_workers=5):\n",
    "    all_q = pd.DataFrame(columns=[\"text\", \"instruction\"])\n",
    "    for i in range(len(contexts)):\n",
    "        context_input = [(contexts[i], n1, n2, max_attempt) for _ in range(m)]\n",
    "        try:\n",
    "            context_q = multiprocess_multiple_variables(genq_by_context, context_input, n_workers)\n",
    "            for j in range(len(context_q)):\n",
    "                all_q.loc[len(all_q)] = [contexts[i], context_q[j]]\n",
    "            print(f\"{i}/{len(contexts)} processed\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"Fail to generate for the {i}-th context! Skipping it...\")\n",
    "    return all_q\n",
    "\n",
    "\n",
    "def gen_all_q_multiprocess(contexts, m, n1=20, n2=20, n_workers=3):\n",
    "    all_q = pd.DataFrame(columns=[\"text\", \"instruction\"])\n",
    "    for _ in tqdm(range(m)):\n",
    "        all_inputs = []\n",
    "        for j in range(len(contexts)):\n",
    "            all_inputs.append((contexts[j], n1, n2))\n",
    "\n",
    "        result = multiprocess(genq_by_context, iter(all_inputs), n_workers)\n",
    "\n",
    "        for r in result:\n",
    "            if result[r] != None:\n",
    "                all_q.loc[len(all_q) + 1] = [r[0], result[r]]\n",
    "    all_q = all_q.sort_values(by='text')\n",
    "    return all_q\n",
    "\n",
    "\n",
    "all_q = gen_all_q_multiprocess(splitted_texts, 3)\n",
    "print(\"time taken for multiprocessing:\", time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/87 processed\n",
      "1/87 processed\n",
      "2/87 processed\n",
      "3/87 processed\n",
      "4/87 processed\n",
      "5/87 processed\n",
      "6/87 processed\n",
      "Rate limite exceeded, sleeping for 5 seconds\n",
      "7/87 processed\n",
      "8/87 processed\n",
      "9/87 processed\n",
      "10/87 processed\n",
      "11/87 processed\n",
      "Rate limite exceeded, sleeping for 5 seconds\n",
      "12/87 processed\n",
      "13/87 processed\n"
     ]
    }
   ],
   "source": [
    "# single processing code\n",
    "start_time = time.time()\n",
    "def gen_m_q_for_n_context(contexts, m, n1=20, n2=20, max_attempt=5):\n",
    "    all_q = pd.DataFrame(columns=[\"text\", \"instruction\"])\n",
    "    for i in range(len(contexts)):\n",
    "        print(f\"{i}/{len(contexts)} processed\")\n",
    "        for _ in range(m):\n",
    "            try:\n",
    "                result = genq_by_context(contexts[i], n1, n2, max_attempt)\n",
    "                if result != None:\n",
    "                    all_q.loc[len(all_q)] = [contexts[i], result]\n",
    "            except:\n",
    "                print(f\"Failed to generate for {i}-th context! Skipping it...\")\n",
    "                break\n",
    "    return all_q\n",
    "\n",
    "all_q = gen_m_q_for_n_context(remaining_texts, 3)\n",
    "print(\"time taken for single processing:\", time.time() - start_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_q.to_csv(\"data/fineweb_instructions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Answers thought LLM agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_q = [\"Who is skywalker?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "instructions = [\"What should I do if I feel depressed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: It seems like a subjective question, but I can try to find some information about popular cat breeds.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Google Search Snippets\",\n",
      "  \"action_input\": \"best cat breed in the world\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'search_results': \"10. Devon Rex Cats · 9. Abyssinian Cats · 8. Sphynx Cats · 7. Scottish Fold Cats · 6. American Shorthair Cats · 5. Maine Coon Cats · 4. Persian Cats · 3. British\\xa0... Sep 14, 2023 ... 1. Ragdoll · 2. Main coon cat · 3. Devon Rex · 4. Exotic shorthair · 5. Persian · 6. British shorthair · 7. Abyssinian · 8. Domestic non-pedigreed cats. Jan 25, 2021 ... Persian cats are so cute, they look like freakin' stuffed animals but they're REAL. They\\xa0...\", 'sources': ['https://www.petinsurance.com/healthzone/pet-breeds/cat-breeds/lifestyle-cat-breeds/top-10-cat-breeds/', 'https://www.forbes.com/advisor/pet-insurance/pet-care/popular-cat-breeds/', 'https://www.cosmopolitan.com/lifestyle/g29824652/best-cat-breeds/']}\u001b[0m\n",
      "Thought:Failed to generate, sleeping for 60 seconds\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 41\u001b[0m, in \u001b[0;36msearch_for_query\u001b[0;34m(query, llm, k)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     sources \u001b[38;5;241m=\u001b[39m search_engine\u001b[38;5;241m.\u001b[39mget_all_resources()\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m     emit_warning()\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain/chains/base.py:600\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    601\u001b[0m         _output_key\n\u001b[1;32m    602\u001b[0m     ]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m     emit_warning()\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain/chains/base.py:383\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    376\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    381\u001b[0m }\n\u001b[0;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain/chains/base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain/chains/base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    155\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    159\u001b[0m )\n\u001b[1;32m    161\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    162\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    163\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain/agents/agent.py:1612\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1611\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1612\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1619\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain/agents/agent.py:1318\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1311\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1316\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1318\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1328\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain/agents/agent.py:1318\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1311\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1316\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1318\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1328\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain/agents/agent.py:1346\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m-> 1346\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1350\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain/agents/agent.py:809\u001b[0m, in \u001b[0;36mAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m full_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_full_inputs(intermediate_steps, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 809\u001b[0m full_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfull_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_parser\u001b[38;5;241m.\u001b[39mparse(full_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain/chains/llm.py:318\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m        completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m     emit_warning()\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain/chains/base.py:383\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    376\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    381\u001b[0m }\n\u001b[0;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain/chains/base.py:166\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    167\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain/chains/base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    155\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    159\u001b[0m )\n\u001b[1;32m    161\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    162\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    163\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain/chains/llm.py:128\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    125\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    126\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 128\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain/chains/llm.py:140\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:714\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    713\u001b[0m prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 714\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:571\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 571\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    572\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    573\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    575\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:561\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    560\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 561\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m            \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:793\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 793\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/langchain_groq/chat_models.py:472\u001b[0m, in \u001b[0;36mChatGroq._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    471\u001b[0m }\n\u001b[0;32m--> 472\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/groq/resources/chat/completions.py:289\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03mCreates a model response for the given chat conversation.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/groq/_base_client.py:1225\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1222\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1223\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1224\u001b[0m )\n\u001b[0;32m-> 1225\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/groq/_base_client.py:920\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    912\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    913\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    918\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    919\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 920\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/groq/_base_client.py:1003\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1002\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1003\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/groq/_base_client.py:1051\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1051\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/groq/_base_client.py:1003\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1002\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1003\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/groq/_base_client.py:1051\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1051\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/groq/_base_client.py:951\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 951\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/ssl.py:1263\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1261\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1262\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nexa/lib/python3.11/ssl.py:1136\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m             attempt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m result, sources \u001b[38;5;241m=\u001b[39m \u001b[43msearch_for_query\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the best cat in the world\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 47\u001b[0m, in \u001b[0;36msearch_for_query\u001b[0;34m(query, llm, k)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to generate, sleeping for 60 seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m     48\u001b[0m         attempt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "from langchain_core.tools import Tool\n",
    "from langchain.agents import initialize_agent\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n",
    "\n",
    "class CustomGoogleSearchWrapper:\n",
    "    def __init__(self, k):\n",
    "        self.all_resources = []\n",
    "        self.search = GoogleSearchAPIWrapper()\n",
    "        self.k = k\n",
    "    def top_k_results(self, query):\n",
    "        res = self.search.results(query, self.k)\n",
    "        # only keep the entries in res if \"snippet\" is in it\n",
    "        res = [r for r in res if \"snippet\" in r]\n",
    "        results = \" \".join([r[\"snippet\"] for r in res])\n",
    "        sources = [r[\"link\"] for r in res]\n",
    "        self.all_resources.extend(sources)\n",
    "        return {\"search_results\" : results, \"sources\" : sources}\n",
    "    def get_all_resources(self):\n",
    "        return self.all_resources\n",
    "    def reset_all_resources(self):\n",
    "        self.all_resources = []\n",
    "\n",
    "def search_for_query(query, llm=None, k=3):\n",
    "    if llm == None:\n",
    "        llm = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n",
    "    search_engine = CustomGoogleSearchWrapper(k)\n",
    "    meta_data_search_tool = Tool(\n",
    "        name=\"Google Search Snippets\",\n",
    "        description=\"Search Google for recent results.\",\n",
    "        func=search_engine.top_k_results,\n",
    "    )\n",
    "    tools = [meta_data_search_tool]\n",
    "    agent = initialize_agent(tools, llm, agent=\"chat-zero-shot-react-description\", verbose=True, agent_kwargs={\"max_execution_time\": 300})\n",
    "    attempt = 0\n",
    "    while attempt < 5:\n",
    "        try:\n",
    "            res = agent.run(query)\n",
    "            sources = search_engine.get_all_resources()\n",
    "            # search_engine.reset_all_resources()\n",
    "            return res, sources \n",
    "        except:\n",
    "            print(\"Failed to generate, sleeping for 60 seconds\")\n",
    "            time.sleep(60)\n",
    "            attempt += 1\n",
    "    return None, None\n",
    "\n",
    "result, sources = search_for_query(\"What is the best cat in the world\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's important to prioritize mental health and seek help if feeling depressed. Strategies to alleviate depression include taking short walks, engaging in physical activity, talking to someone about feelings, and practicing healthy habits. Depression can manifest in various ways, including feelings of sadness, emptiness, and hopelessness, as well as physical symptoms such as joint pain and sleep trouble. To overcome depression, one can try moderate exercise, therapy, and lifestyle changes, and seek professional help if symptoms persist.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>instruction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In an increasingly complex financial world, eq...</td>\n",
       "      <td>How can a parent encourage their 12-year-old t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In an increasingly complex financial world, eq...</td>\n",
       "      <td>As a financial literacy expert, how can you he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In an increasingly complex financial world, eq...</td>\n",
       "      <td>How can a parent encourage their 12-year-old c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is Decentralized Finance (DeFi)? A Compre...</td>\n",
       "      <td>How might the development of decentralized aut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is Decentralized Finance (DeFi)? A Compre...</td>\n",
       "      <td>Can you compare the economic impact of a decen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is Decentralized Finance (DeFi)? A Compre...</td>\n",
       "      <td>How might the development of decentralized aut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Running a successful business depends on how w...</td>\n",
       "      <td>How would a financial analyst recommend adjust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Running a successful business depends on how w...</td>\n",
       "      <td>A company is experiencing a cash crunch due to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Running a successful business depends on how w...</td>\n",
       "      <td>What are the potential consequences of not pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is Business Credit?\\nBusiness credit refe...</td>\n",
       "      <td>Can a small business owner with a limited cred...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  In an increasingly complex financial world, eq...   \n",
       "1  In an increasingly complex financial world, eq...   \n",
       "2  In an increasingly complex financial world, eq...   \n",
       "3  What is Decentralized Finance (DeFi)? A Compre...   \n",
       "4  What is Decentralized Finance (DeFi)? A Compre...   \n",
       "5  What is Decentralized Finance (DeFi)? A Compre...   \n",
       "6  Running a successful business depends on how w...   \n",
       "7  Running a successful business depends on how w...   \n",
       "8  Running a successful business depends on how w...   \n",
       "9  What is Business Credit?\\nBusiness credit refe...   \n",
       "\n",
       "                                         instruction  \n",
       "0  How can a parent encourage their 12-year-old t...  \n",
       "1  As a financial literacy expert, how can you he...  \n",
       "2  How can a parent encourage their 12-year-old c...  \n",
       "3  How might the development of decentralized aut...  \n",
       "4  Can you compare the economic impact of a decen...  \n",
       "5  How might the development of decentralized aut...  \n",
       "6  How would a financial analyst recommend adjust...  \n",
       "7  A company is experiencing a cash crunch due to...  \n",
       "8  What are the potential consequences of not pro...  \n",
       "9  Can a small business owner with a limited cred...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction_df = pd.read_csv(\"data/fineweb_instructions.csv\")\n",
    "instruction_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:21<00:00,  4.07s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def search_for_all_queries(instruction_df, original_query):\n",
    "    llm = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n",
    "    completed_df = pd.DataFrame(columns = [\"original_context\", \"instruction\", \"response\", \"sources\", \"original_query\"])\n",
    "    all_contexts = list(instruction_df[\"text\"])\n",
    "    all_instructions = list(instruction_df[\"instruction\"])\n",
    "    for i in tqdm(range(len(instruction_df))):\n",
    "        res, sources = search_for_query(all_instructions[i], llm)\n",
    "        if res != None and len(res) >=80 and \"action_input\" not in res:\n",
    "            if \"The final answer to the question is:\" in res and res.index(\"The final answer to the question is:\") == 0:\n",
    "                res = res[len(\"The final answer to the question is:\"):].strip()\n",
    "            if \"The final answer to the original input question is:\" in res and res.index(\"The final answer to the original input question is:\") == 0:\n",
    "                res = res[len(\"The final answer to the original input question is:\"):].strip()\n",
    "            if \"The final answer to the original input question.\" in res and res.index(\"The final answer to the original input question.\") == 0:\n",
    "                res = res[len(\"The final answer to the original input question.\"):].strip()\n",
    "            completed_df.loc[len(completed_df) + 1] = [all_contexts[i], all_instructions[i], res, sources, original_query]\n",
    "    return completed_df\n",
    "\n",
    "\n",
    "completed_df = search_for_all_queries(instruction_df[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_context</th>\n",
       "      <th>instruction</th>\n",
       "      <th>response</th>\n",
       "      <th>sources</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In an increasingly complex financial world, eq...</td>\n",
       "      <td>How can a parent encourage their 12-year-old t...</td>\n",
       "      <td>To encourage a 12-year-old to save for a long-...</td>\n",
       "      <td>[https://529.wa.gov/, https://www.mefa.org/blo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In an increasingly complex financial world, eq...</td>\n",
       "      <td>As a financial literacy expert, how can you he...</td>\n",
       "      <td>To help a young adult develop a savings plan t...</td>\n",
       "      <td>[https://www.nerdwallet.com/article/mortgages/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In an increasingly complex financial world, eq...</td>\n",
       "      <td>How can a parent encourage their 12-year-old c...</td>\n",
       "      <td></td>\n",
       "      <td>[https://www.ramseysolutions.com/relationships...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is Decentralized Finance (DeFi)? A Compre...</td>\n",
       "      <td>How might the development of decentralized aut...</td>\n",
       "      <td>The development of decentralized autonomous or...</td>\n",
       "      <td>[https://home.treasury.gov/system/files/136/De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is Decentralized Finance (DeFi)? A Compre...</td>\n",
       "      <td>Can you compare the economic impact of a decen...</td>\n",
       "      <td>The economic impact of a decentralized financi...</td>\n",
       "      <td>[https://www.fsb.org/wp-content/uploads/P06061...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is Decentralized Finance (DeFi)? A Compre...</td>\n",
       "      <td>How might the development of decentralized aut...</td>\n",
       "      <td>The development of decentralized autonomous or...</td>\n",
       "      <td>[https://home.treasury.gov/system/files/136/De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Running a successful business depends on how w...</td>\n",
       "      <td>How would a financial analyst recommend adjust...</td>\n",
       "      <td>The financial analyst would recommend adjustin...</td>\n",
       "      <td>[https://disclosure.spglobal.com/en/regulatory...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Running a successful business depends on how w...</td>\n",
       "      <td>A company is experiencing a cash crunch due to...</td>\n",
       "      <td>To improve its cash flow situation, the compan...</td>\n",
       "      <td>[https://upflow.io/blog/ar-collections/account...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Running a successful business depends on how w...</td>\n",
       "      <td>What are the potential consequences of not pro...</td>\n",
       "      <td>The consequences of not properly managing a co...</td>\n",
       "      <td>[https://www.linkedin.com/advice/0/what-risks-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What is Business Credit?\\nBusiness credit refe...</td>\n",
       "      <td>Can a small business owner with a limited cred...</td>\n",
       "      <td>Yes, a small business owner with a limited cre...</td>\n",
       "      <td>[https://www.ffiec.gov/hmda/pdf/2021Guide.pdf,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What is Business Credit?\\nBusiness credit refe...</td>\n",
       "      <td>Can a small business owner use business credit...</td>\n",
       "      <td>The final answer to the original input questio...</td>\n",
       "      <td>[https://www.sba.gov/business-guide/plan-your-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What is Business Credit?\\nBusiness credit refe...</td>\n",
       "      <td>What are some common mistakes that small busin...</td>\n",
       "      <td>Establishing a strong business credit profile ...</td>\n",
       "      <td>[https://www.quora.com/What-is-the-worst-mista...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Basic Guide To Business Process And Busine...</td>\n",
       "      <td>What strategy can a company use to improve its...</td>\n",
       "      <td>To improve customer satisfaction score by 20% ...</td>\n",
       "      <td>[https://kaizo.com/blog/improve-customer-satis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The Basic Guide To Business Process And Busine...</td>\n",
       "      <td>How can a company effectively combine its mark...</td>\n",
       "      <td>To effectively combine marketing and sales tea...</td>\n",
       "      <td>[https://community.hubspot.com/t5/Tips-Tricks-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The Basic Guide To Business Process And Busine...</td>\n",
       "      <td>Can you recommend a strategy to streamline a c...</td>\n",
       "      <td>To streamline a company's customer service ope...</td>\n",
       "      <td>[https://www.startingpoint.ai/post/15-ways-str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What is Finance?\\nIs it worth it? This is the ...</td>\n",
       "      <td>What is the optimal mix of investments for a p...</td>\n",
       "      <td>The optimal mix of investments for a pension f...</td>\n",
       "      <td>[https://www.schwab.com/learn/story/structurin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What is Finance?\\nIs it worth it? This is the ...</td>\n",
       "      <td>Can a company's valuation multiples justify a ...</td>\n",
       "      <td>To determine if a company's valuation multiple...</td>\n",
       "      <td>[https://www.fe.training/free-resources/ma/val...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What is Finance?\\nIs it worth it? This is the ...</td>\n",
       "      <td>What strategic factors should a company consid...</td>\n",
       "      <td>The strategic factors to consider when decidin...</td>\n",
       "      <td>[https://mconsultingprep.com/market-entry-fram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>When it comes to pursuing a career in the busi...</td>\n",
       "      <td>What are the key factors that influence an org...</td>\n",
       "      <td>The key factors that influence an organization...</td>\n",
       "      <td>[https://www.weforum.org/publications/the-futu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>When it comes to pursuing a career in the busi...</td>\n",
       "      <td>What are the key factors that an employer cons...</td>\n",
       "      <td>The key factors that an employer considers whe...</td>\n",
       "      <td>[https://www.opm.gov/policy-data-oversight/cla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     original_context  \\\n",
       "1   In an increasingly complex financial world, eq...   \n",
       "2   In an increasingly complex financial world, eq...   \n",
       "3   In an increasingly complex financial world, eq...   \n",
       "4   What is Decentralized Finance (DeFi)? A Compre...   \n",
       "5   What is Decentralized Finance (DeFi)? A Compre...   \n",
       "6   What is Decentralized Finance (DeFi)? A Compre...   \n",
       "7   Running a successful business depends on how w...   \n",
       "8   Running a successful business depends on how w...   \n",
       "9   Running a successful business depends on how w...   \n",
       "10  What is Business Credit?\\nBusiness credit refe...   \n",
       "11  What is Business Credit?\\nBusiness credit refe...   \n",
       "12  What is Business Credit?\\nBusiness credit refe...   \n",
       "13  The Basic Guide To Business Process And Busine...   \n",
       "14  The Basic Guide To Business Process And Busine...   \n",
       "15  The Basic Guide To Business Process And Busine...   \n",
       "16  What is Finance?\\nIs it worth it? This is the ...   \n",
       "17  What is Finance?\\nIs it worth it? This is the ...   \n",
       "18  What is Finance?\\nIs it worth it? This is the ...   \n",
       "19  When it comes to pursuing a career in the busi...   \n",
       "20  When it comes to pursuing a career in the busi...   \n",
       "\n",
       "                                          instruction  \\\n",
       "1   How can a parent encourage their 12-year-old t...   \n",
       "2   As a financial literacy expert, how can you he...   \n",
       "3   How can a parent encourage their 12-year-old c...   \n",
       "4   How might the development of decentralized aut...   \n",
       "5   Can you compare the economic impact of a decen...   \n",
       "6   How might the development of decentralized aut...   \n",
       "7   How would a financial analyst recommend adjust...   \n",
       "8   A company is experiencing a cash crunch due to...   \n",
       "9   What are the potential consequences of not pro...   \n",
       "10  Can a small business owner with a limited cred...   \n",
       "11  Can a small business owner use business credit...   \n",
       "12  What are some common mistakes that small busin...   \n",
       "13  What strategy can a company use to improve its...   \n",
       "14  How can a company effectively combine its mark...   \n",
       "15  Can you recommend a strategy to streamline a c...   \n",
       "16  What is the optimal mix of investments for a p...   \n",
       "17  Can a company's valuation multiples justify a ...   \n",
       "18  What strategic factors should a company consid...   \n",
       "19  What are the key factors that influence an org...   \n",
       "20  What are the key factors that an employer cons...   \n",
       "\n",
       "                                             response  \\\n",
       "1   To encourage a 12-year-old to save for a long-...   \n",
       "2   To help a young adult develop a savings plan t...   \n",
       "3                                                       \n",
       "4   The development of decentralized autonomous or...   \n",
       "5   The economic impact of a decentralized financi...   \n",
       "6   The development of decentralized autonomous or...   \n",
       "7   The financial analyst would recommend adjustin...   \n",
       "8   To improve its cash flow situation, the compan...   \n",
       "9   The consequences of not properly managing a co...   \n",
       "10  Yes, a small business owner with a limited cre...   \n",
       "11  The final answer to the original input questio...   \n",
       "12  Establishing a strong business credit profile ...   \n",
       "13  To improve customer satisfaction score by 20% ...   \n",
       "14  To effectively combine marketing and sales tea...   \n",
       "15  To streamline a company's customer service ope...   \n",
       "16  The optimal mix of investments for a pension f...   \n",
       "17  To determine if a company's valuation multiple...   \n",
       "18  The strategic factors to consider when decidin...   \n",
       "19  The key factors that influence an organization...   \n",
       "20  The key factors that an employer considers whe...   \n",
       "\n",
       "                                              sources  \n",
       "1   [https://529.wa.gov/, https://www.mefa.org/blo...  \n",
       "2   [https://www.nerdwallet.com/article/mortgages/...  \n",
       "3   [https://www.ramseysolutions.com/relationships...  \n",
       "4   [https://home.treasury.gov/system/files/136/De...  \n",
       "5   [https://www.fsb.org/wp-content/uploads/P06061...  \n",
       "6   [https://home.treasury.gov/system/files/136/De...  \n",
       "7   [https://disclosure.spglobal.com/en/regulatory...  \n",
       "8   [https://upflow.io/blog/ar-collections/account...  \n",
       "9   [https://www.linkedin.com/advice/0/what-risks-...  \n",
       "10  [https://www.ffiec.gov/hmda/pdf/2021Guide.pdf,...  \n",
       "11  [https://www.sba.gov/business-guide/plan-your-...  \n",
       "12  [https://www.quora.com/What-is-the-worst-mista...  \n",
       "13  [https://kaizo.com/blog/improve-customer-satis...  \n",
       "14  [https://community.hubspot.com/t5/Tips-Tricks-...  \n",
       "15  [https://www.startingpoint.ai/post/15-ways-str...  \n",
       "16  [https://www.schwab.com/learn/story/structurin...  \n",
       "17  [https://www.fe.training/free-resources/ma/val...  \n",
       "18  [https://mconsultingprep.com/market-entry-fram...  \n",
       "19  [https://www.weforum.org/publications/the-futu...  \n",
       "20  [https://www.opm.gov/policy-data-oversight/cla...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Upload of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/orbina/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68d81e2aea140c6b961eb167907773e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd947b6cc972415380e4fb00702c7991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24110d150af84587bbf7455d782a2cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6edf6f354f4e87ae20d2e1b1cb9a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/824 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/bs82/NexaAIHackathonDemo/commit/f75e0e74252b6ca39decff874ec640a7a98f870d', commit_message='Upload dataset', commit_description='', oid='f75e0e74252b6ca39decff874ec640a7a98f870d', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "login(os.environ[\"HUGGINGFACE_TOKEN\"])\n",
    "\n",
    "folder_path = \"demo_result\"\n",
    "\n",
    "data_files = {\n",
    "    \"train\": [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "}\n",
    "\n",
    "dataset_dict = load_dataset(\"csv\", data_files=data_files)\n",
    "dataset_dict.push_to_hub(\"bs82/NexaAIHackathonDemo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nexa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
